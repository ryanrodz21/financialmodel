{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "isInteractiveWindowMessageCell": true
   },
   "source": [
    "Connected to base (Python 3.11.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NewsAPIException",
     "evalue": "{'status': 'error', 'code': 'parameterInvalid', 'message': 'You are trying to request results too far in the past. Your plan permits you to request articles as far back as 2024-12-30, but you have requested 2024-11-25. You may need to upgrade to a paid plan.'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNewsAPIException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py\u001b[0m in \u001b[0;36mline 107\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=102'>103</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mTop 10 influential\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=105'>106</a>\u001b[0m \u001b[39m# Fetch news for debugging purposes\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=106'>107</a>\u001b[0m fetch_news(\u001b[39m'\u001b[39m\u001b[39mTSLA\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=108'>109</a>\u001b[0m \u001b[39m# Call the main function\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=109'>110</a>\u001b[0m execute(\u001b[39m'\u001b[39m\u001b[39mTSLA\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m2025-2-26\u001b[39m\u001b[39m'\u001b[39m, sentiment_scaling_factor\u001b[39m=\u001b[39m\u001b[39m10000.0\u001b[39m)\n",
      "\u001b[1;32m/Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py\u001b[0m in \u001b[0;36mline 27\u001b[0m, in \u001b[0;36mfetch_news\u001b[0;34m(ticker, company_name)\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=23'>24</a>\u001b[0m \u001b[39mif\u001b[39;00m company_name:\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=24'>25</a>\u001b[0m     query \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m OR \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mcompany_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=26'>27</a>\u001b[0m all_articles \u001b[39m=\u001b[39m newsapi\u001b[39m.\u001b[39mget_everything(q\u001b[39m=\u001b[39mquery,\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=27'>28</a>\u001b[0m                                       from_param\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m2024-11-25\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=28'>29</a>\u001b[0m                                       to\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m2024-12-06\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=29'>30</a>\u001b[0m                                       language\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39men\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=30'>31</a>\u001b[0m                                       sort_by\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelevancy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=31'>32</a>\u001b[0m news_data \u001b[39m=\u001b[39m all_articles\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39marticles\u001b[39m\u001b[39m'\u001b[39m, [])\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=33'>34</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m news_data:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/newsapi/newsapi_client.py:334\u001b[0m, in \u001b[0;36mNewsApiClient.get_everything\u001b[0;34m(self, q, qintitle, sources, domains, exclude_domains, from_param, to, language, sort_by, page, page_size)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/newsapi/newsapi_client.py?line=331'>332</a>\u001b[0m \u001b[39m# Check Status of Request\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/newsapi/newsapi_client.py?line=332'>333</a>\u001b[0m \u001b[39mif\u001b[39;00m r\u001b[39m.\u001b[39mstatus_code \u001b[39m!=\u001b[39m requests\u001b[39m.\u001b[39mcodes\u001b[39m.\u001b[39mok:\n\u001b[0;32m--> <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/newsapi/newsapi_client.py?line=333'>334</a>\u001b[0m     \u001b[39mraise\u001b[39;00m NewsAPIException(r\u001b[39m.\u001b[39mjson())\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/newsapi/newsapi_client.py?line=335'>336</a>\u001b[0m \u001b[39mreturn\u001b[39;00m r\u001b[39m.\u001b[39mjson()\n",
      "\u001b[0;31mNewsAPIException\u001b[0m: {'status': 'error', 'code': 'parameterInvalid', 'message': 'You are trying to request results too far in the past. Your plan permits you to request articles as far back as 2024-12-30, but you have requested 2024-11-25. You may need to upgrade to a paid plan.'}"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from newsapi import NewsApiClient\n",
    "\n",
    "# Initializing the sentiment intensity analyzer from NLTK\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Initializing the NewsAPI client with API key\n",
    "newsapi = NewsApiClient(api_key='ec064ce719114fe78bd3affdd71e5db8')  # Replace with your actual API key\n",
    "\n",
    "def fetch_data(ticker):\n",
    "    # Fetch 20 years of historical stock data for the provided ticker using yfinance\n",
    "    t = yf.Ticker(ticker)\n",
    "    historical_data = t.history(period=\"20y\")\n",
    "    historical_data.reset_index(inplace=True)\n",
    "    historical_data['Date'] = historical_data['Date'].dt.tz_localize(None)\n",
    "    return historical_data\n",
    "\n",
    "def fetch_news(ticker, company_name=None):\n",
    "    query = f'\"{ticker}\"'\n",
    "    if company_name:\n",
    "        query += f' OR \"{company_name}\"'\n",
    "    \n",
    "    all_articles = newsapi.get_everything(q=query,\n",
    "                                          from_param='2024-11-25',\n",
    "                                          to='2024-12-06',\n",
    "                                          language='en',\n",
    "                                          sort_by='relevancy')\n",
    "    news_data = all_articles.get('articles', [])\n",
    "    \n",
    "    if not news_data:\n",
    "        print(f\"No articles found for ticker: {ticker}\")\n",
    "        return pd.DataFrame(columns=['Date', 'title'])\n",
    "\n",
    "    news_df = pd.DataFrame(news_data)\n",
    "    \n",
    "    # Converting publication timestamp to datetime for consistency\n",
    "    news_df['Date'] = pd.to_datetime(news_df['publishedAt']).dt.date\n",
    "    news_df['Date'] = pd.to_datetime(news_df['Date'])\n",
    "    \n",
    "    return news_df[['Date', 'title']]\n",
    "\n",
    "def extract_sentiment(news_df, sentiment_scaling_factor):\n",
    "    news_df['title_sentiment'] = news_df['title'].apply(lambda x: sid.polarity_scores(x)['compound'] * sentiment_scaling_factor)\n",
    "    return news_df\n",
    "\n",
    "def merge_news_with_data(data, news):\n",
    "    # Ensuring 'Date' column in both dataframes is of type datetime\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    news['Date'] = pd.to_datetime(news['Date'])\n",
    "\n",
    "    merged_data = data.merge(news, on='Date', how='left')\n",
    "    merged_data.fillna(0, inplace=True)\n",
    "    return merged_data\n",
    "\n",
    "def process_data(data, look_back=250):\n",
    "    for i in range(1, look_back + 1):\n",
    "        col_name = f\"lag_{i}\"\n",
    "        data[col_name] = data['Close'].shift(i)\n",
    "        \n",
    "    data.dropna(inplace=True)\n",
    "    X = data.drop(['Close', 'Date', 'title'], axis=1)\n",
    "    y = data['Close']\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    X_scaled = scaler_x.fit_transform(X)\n",
    "    y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n",
    "    return pd.DataFrame(X_scaled, columns=X.columns), pd.Series(y_scaled.ravel()), scaler_x, scaler_y\n",
    "\n",
    "def execute(ticker, target_date, sentiment_scaling_factor=10000.0):\n",
    "    look_back = 60\n",
    "    data = fetch_data(ticker)\n",
    "    news = fetch_news(ticker)\n",
    "    news = extract_sentiment(news, sentiment_scaling_factor)\n",
    "    data = merge_news_with_data(data, news)\n",
    "    X, y, scaler_x, scaler_y = process_data(data, look_back)\n",
    "    \n",
    "    column_names = X.columns.tolist()\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators=100)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    target_data = X.iloc[-1].values.reshape(1, -1)\n",
    "    pred_scaled = model.predict(target_data)\n",
    "    pred = scaler_y.inverse_transform([[pred_scaled[0]]])[0, 0]\n",
    "    print(f\"\\nThe predicted closing price for {ticker} on {target_date} is {pred}\")\n",
    "\n",
    "    last_known_price = y.iloc[-1]\n",
    "    if pred > last_known_price:\n",
    "        print(f\"The model suggests buying a CALL option for {ticker} expiring on {target_date}.\")\n",
    "    elif pred < last_known_price:\n",
    "        print(f\"The model suggests buying a PUT option for {ticker} expiring on {target_date}.\")\n",
    "    else:\n",
    "        print(f\"The model suggests no clear direction for {ticker} on {target_date}.\")\n",
    "\n",
    "    last_month = pd.Timestamp(target_date) - pd.DateOffset(months=1)\n",
    "    filtered_data = data[data['Date'] > last_month]\n",
    "    data_sorted_by_sentiment = filtered_data.sort_values(by='title_sentiment', key=abs, ascending=False)\n",
    "    top_10_news = data_sorted_by_sentiment[['Date', 'title', 'title_sentiment']].head(10)\n",
    "    print(\"\\nTop 10 influential\")\n",
    "\n",
    "\n",
    "# Fetch news for debugging purposes\n",
    "fetch_news('TSLA')\n",
    "\n",
    "# Call the main function\n",
    "execute('TSLA', '2025-2-26', sentiment_scaling_factor=10000.0)\n",
    "\n",
    "#%%\n",
    "\n",
    "#   BERT SENTIMENT MODEL\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from newsapi import NewsApiClient\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize sentiment analysis using BERT from transformers\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "newsapi = NewsApiClient(api_key='ec064ce719114fe78bd3affdd71e5db8')  \n",
    "\n",
    "def fetch_data(ticker):\n",
    "    # Fetch 20 years of historical data for the given ticker using yfinance\n",
    "    t = yf.Ticker(ticker)\n",
    "    historical_data = t.history(period=\"20y\")\n",
    "    historical_data.reset_index(inplace=True)\n",
    "    # Remove timezone information to make it consistent with other datetime objects\n",
    "    historical_data['Date'] = historical_data['Date'].dt.tz_localize(None)\n",
    "    return historical_data\n",
    "\n",
    "def fetch_news(ticker):\n",
    "    # Fetch news articles for the given ticker for the past month\n",
    "    all_articles = newsapi.get_everything(q=f\"{ticker}\",\n",
    "    from_param='2024-12-07',\n",
    "    to='2024-12-30',\n",
    "    language='en',\n",
    "    sort_by='relevancy')\n",
    "\n",
    "    news_data = all_articles.get('articles', [])\n",
    "    \n",
    "    if not news_data:\n",
    "        print(f\"No articles found for ticker: {ticker}\")\n",
    "        return pd.DataFrame(columns=['Date', 'title'])\n",
    "\n",
    "    news_df = pd.DataFrame(news_data)\n",
    "    # Remove timezone information to make it consistent with other datetime objects\n",
    "    news_df['Date'] = pd.to_datetime(news_df['publishedAt']).dt.tz_localize(None)\n",
    "    return news_df[['Date', 'title', 'description', 'content']]\n",
    "\n",
    "def extract_sentiment(news_df, sentiment_weight=1000.0):\n",
    "    # Combine title, description, and content for better sentiment analysis\n",
    "    news_df['combined_text'] = news_df['title'] + ' ' + news_df.get('description', '') + ' ' + news_df.get('content', '')\n",
    "\n",
    "    # Use BERT sentiment analysis to extract sentiment from the combined text\n",
    "    news_df['title_sentiment'] = news_df['combined_text'].apply(lambda x: sentiment_weight if sentiment_analysis(x)[0]['label'] == 'POSITIVE' else (-sentiment_weight if sentiment_analysis(x)[0]['label'] == 'NEGATIVE' else 0))\n",
    "    \n",
    "    return news_df[['Date', 'title', 'title_sentiment', 'combined_text']]\n",
    "\n",
    "def merge_news_with_data(data, news):\n",
    "    # Merge historical stock data with news data on Date\n",
    "    merged_data = data.merge(news, on='Date', how='left')\n",
    "    merged_data.fillna(0, inplace=True)\n",
    "    # Calculate a weighted sentiment score based on sentiment and volume\n",
    "    merged_data['weighted_sentiment'] = merged_data['title_sentiment'] * merged_data['Volume']\n",
    "    return merged_data\n",
    "\n",
    "def process_data(data, look_back=250):\n",
    "    # Create a list to hold all the lag features\n",
    "    lag_columns = []\n",
    "\n",
    "    # Generate lag features and append them to the list\n",
    "    for i in range(1, look_back + 1):\n",
    "        lag_column = data['Close'].shift(i)\n",
    "        lag_column.name = f\"lag_{i}\"\n",
    "        lag_columns.append(lag_column)\n",
    "\n",
    "    # Combine all lag features into a single DataFrame\n",
    "    lag_features = pd.concat(lag_columns, axis=1)\n",
    "\n",
    "    # Combine the original data with the lag features\n",
    "    data = pd.concat([data, lag_features], axis=1)\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    # Define columns that are not features for the model\n",
    "    columns_to_drop = ['Close', 'Date', 'title', 'combined_text']\n",
    "    \n",
    "    # Split data into features and target variable\n",
    "    X = data.drop(columns_to_drop, axis=1)\n",
    "    y = data['Close']\n",
    "\n",
    "    # Normalize feature data and target data\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    X_scaled = scaler_x.fit_transform(X)\n",
    "    y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n",
    "    return pd.DataFrame(X_scaled, columns=X.columns), pd.Series(y_scaled.ravel()), scaler_x, scaler_y\n",
    "\n",
    "\n",
    "\n",
    "def execute(ticker, target_date, sentiment_weight=100000.0):\n",
    "    # Function to execute the entire pipeline\n",
    "    data = fetch_data(ticker)\n",
    "    news = fetch_news(ticker)\n",
    "    news = extract_sentiment(news, sentiment_weight)\n",
    "    data = merge_news_with_data(data, news)\n",
    "    X, y, scaler_x, scaler_y = process_data(data)\n",
    "\n",
    "    # Train a random forest regressor with the processed data\n",
    "    column_names = X.columns.tolist()\n",
    "    model = RandomForestRegressor(n_estimators=100)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Predict the price for the target date\n",
    "    target_data = X.iloc[-1].values.reshape(1, -1)\n",
    "    pred_scaled = model.predict(target_data)\n",
    "    pred = scaler_y.inverse_transform([[pred_scaled[0]]])[0, 0]\n",
    "    print(f\"\\nThe predicted closing price for {ticker} on {target_date} is {pred}\")\n",
    "    \n",
    "    # Provide a trading suggestion based on the prediction\n",
    "    last_known_price = y.iloc[-1]\n",
    "    if pred > last_known_price:\n",
    "        print(f\"The model suggests buying a CALL option for {ticker} expiring on {target_date}.\")\n",
    "    elif pred < last_known_price:\n",
    "        print(f\"The model suggests buying a PUT option for {ticker} expiring on {target_date}.\")\n",
    "    else:\n",
    "        print(f\"The model suggests no clear direction for {ticker} on {target_date}.\")\n",
    "\n",
    "    # Display importance of sentiment in the model\n",
    "    importance = model.feature_importances_\n",
    "    if 'title_sentiment' in column_names:\n",
    "        sentiment_index = column_names.index('title_sentiment')\n",
    "        sentiment_importance = importance[sentiment_index]\n",
    "        print(f\"\\nImportance of title_sentiment: {sentiment_importance}\")\n",
    "\n",
    "    # Display top 3 positive news articles\n",
    "    positive_news = news[news['title_sentiment'] == sentiment_weight].sort_values(by=\"Date\", ascending=False).head(3)\n",
    "    print(\"\\nTop 3 Positive News Articles (with combined text):\")\n",
    "    for idx, row in positive_news.iterrows():\n",
    "        print(\"\\nTitle:\", row['title'])\n",
    "        print(\"Combined Text:\", row['combined_text'])\n",
    "\n",
    "# Call the function with sentiment_weight\n",
    "execute('TSLA', '2025-12-26', sentiment_weight=100000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NewsAPIException",
     "evalue": "{'status': 'error', 'code': 'parameterInvalid', 'message': 'You are trying to request results too far in the past. Your plan permits you to request articles as far back as 2024-12-30, but you have requested 2024-11-25. You may need to upgrade to a paid plan.'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNewsAPIException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py\u001b[0m in \u001b[0;36mline 93\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=89'>90</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mTop 10 influential\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=90'>91</a>\u001b[0m \u001b[39m# Fetch news for debugging purposes\u001b[39;00m\n\u001b[0;32m---> <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=92'>93</a>\u001b[0m fetch_news(\u001b[39m'\u001b[39m\u001b[39mTSLA\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=93'>94</a>\u001b[0m \u001b[39m# Call the main function\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=95'>96</a>\u001b[0m execute(\u001b[39m'\u001b[39m\u001b[39mTSLA\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m2025-2-26\u001b[39m\u001b[39m'\u001b[39m, sentiment_scaling_factor\u001b[39m=\u001b[39m\u001b[39m10000.0\u001b[39m)\n",
      "\u001b[1;32m/Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py\u001b[0m in \u001b[0;36mline 25\u001b[0m, in \u001b[0;36mfetch_news\u001b[0;34m(ticker, company_name)\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=22'>23</a>\u001b[0m \u001b[39mif\u001b[39;00m company_name:\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=23'>24</a>\u001b[0m     query \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m OR \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mcompany_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=24'>25</a>\u001b[0m all_articles \u001b[39m=\u001b[39m newsapi\u001b[39m.\u001b[39mget_everything(q\u001b[39m=\u001b[39mquery,\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=25'>26</a>\u001b[0m                                       from_param\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m2024-11-25\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=26'>27</a>\u001b[0m                                       to\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m2024-12-06\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=27'>28</a>\u001b[0m                                       language\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39men\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=28'>29</a>\u001b[0m                                       sort_by\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelevancy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=29'>30</a>\u001b[0m news_data \u001b[39m=\u001b[39m all_articles\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39marticles\u001b[39m\u001b[39m'\u001b[39m, [])\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=30'>31</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m news_data:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/newsapi/newsapi_client.py:334\u001b[0m, in \u001b[0;36mNewsApiClient.get_everything\u001b[0;34m(self, q, qintitle, sources, domains, exclude_domains, from_param, to, language, sort_by, page, page_size)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/newsapi/newsapi_client.py?line=331'>332</a>\u001b[0m \u001b[39m# Check Status of Request\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/newsapi/newsapi_client.py?line=332'>333</a>\u001b[0m \u001b[39mif\u001b[39;00m r\u001b[39m.\u001b[39mstatus_code \u001b[39m!=\u001b[39m requests\u001b[39m.\u001b[39mcodes\u001b[39m.\u001b[39mok:\n\u001b[0;32m--> <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/newsapi/newsapi_client.py?line=333'>334</a>\u001b[0m     \u001b[39mraise\u001b[39;00m NewsAPIException(r\u001b[39m.\u001b[39mjson())\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/newsapi/newsapi_client.py?line=335'>336</a>\u001b[0m \u001b[39mreturn\u001b[39;00m r\u001b[39m.\u001b[39mjson()\n",
      "\u001b[0;31mNewsAPIException\u001b[0m: {'status': 'error', 'code': 'parameterInvalid', 'message': 'You are trying to request results too far in the past. Your plan permits you to request articles as far back as 2024-12-30, but you have requested 2024-11-25. You may need to upgrade to a paid plan.'}"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from newsapi import NewsApiClient\n",
    "# Initializing the sentiment intensity analyzer from NLTK\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "# Initializing the NewsAPI client with API key\n",
    "\n",
    "newsapi = NewsApiClient(api_key='ec064ce719114fe78bd3affdd71e5db8')  # Replace with your actual API key\n",
    "def fetch_data(ticker):\n",
    "    # Fetch 20 years of historical stock data for the provided ticker using yfinance\n",
    "    t = yf.Ticker(ticker)\n",
    "    historical_data = t.history(period=\"20y\")\n",
    "    historical_data.reset_index(inplace=True)\n",
    "    historical_data['Date'] = historical_data['Date'].dt.tz_localize(None)\n",
    "    return historical_data\n",
    "\n",
    "def fetch_news(ticker, company_name=None):\n",
    "    query = f'\"{ticker}\"'\n",
    "    if company_name:\n",
    "        query += f' OR \"{company_name}\"'\n",
    "    all_articles = newsapi.get_everything(q=query,\n",
    "                                          from_param='2024-11-25',\n",
    "                                          to='2024-12-06',\n",
    "                                          language='en',\n",
    "                                          sort_by='relevancy')\n",
    "    news_data = all_articles.get('articles', [])\n",
    "    if not news_data:\n",
    "        print(f\"No articles found for ticker: {ticker}\")\n",
    "        return pd.DataFrame(columns=['Date', 'title'])\n",
    "    news_df = pd.DataFrame(news_data)\n",
    "    # Converting publication timestamp to datetime for consistency\n",
    "    news_df['Date'] = pd.to_datetime(news_df['publishedAt']).dt.date\n",
    "    news_df['Date'] = pd.to_datetime(news_df['Date'])\n",
    "    return news_df[['Date', 'title']]\n",
    "\n",
    "def extract_sentiment(news_df, sentiment_scaling_factor):\n",
    "    news_df['title_sentiment'] = news_df['title'].apply(lambda x: sid.polarity_scores(x)['compound'] * sentiment_scaling_factor)\n",
    "    return news_df\n",
    "\n",
    "def merge_news_with_data(data, news):\n",
    "    # Ensuring 'Date' column in both dataframes is of type datetime\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    news['Date'] = pd.to_datetime(news['Date'])\n",
    "    merged_data = data.merge(news, on='Date', how='left')\n",
    "    merged_data.fillna(0, inplace=True)\n",
    "    return merged_data\n",
    "\n",
    "def process_data(data, look_back=250):\n",
    "    for i in range(1, look_back + 1):\n",
    "        col_name = f\"lag_{i}\"\n",
    "        data[col_name] = data['Close'].shift(i)\n",
    "    data.dropna(inplace=True)\n",
    "    X = data.drop(['Close', 'Date', 'title'], axis=1)\n",
    "    y = data['Close']\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    X_scaled = scaler_x.fit_transform(X)\n",
    "    y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n",
    "    return pd.DataFrame(X_scaled, columns=X.columns), pd.Series(y_scaled.ravel()), scaler_x, scaler_y\n",
    "\n",
    "def execute(ticker, target_date, sentiment_scaling_factor=10000.0):\n",
    "    look_back = 30\n",
    "    data = fetch_data(ticker)\n",
    "    news = fetch_news(ticker)\n",
    "    news = extract_sentiment(news, sentiment_scaling_factor)\n",
    "    data = merge_news_with_data(data, news)\n",
    "    X, y, scaler_x, scaler_y = process_data(data, look_back)\n",
    "    column_names = X.columns.tolist()\n",
    "    model = RandomForestRegressor(n_estimators=100)\n",
    "    model.fit(X, y)\n",
    "    target_data = X.iloc[-1].values.reshape(1, -1)\n",
    "    pred_scaled = model.predict(target_data)\n",
    "    pred = scaler_y.inverse_transform([[pred_scaled[0]]])[0, 0]\n",
    "    print(f\"\\nThe predicted closing price for {ticker} on {target_date} is {pred}\")\n",
    "    last_known_price = y.iloc[-1]\n",
    "    if pred > last_known_price:\n",
    "        print(f\"The model suggests buying a CALL option for {ticker} expiring on {target_date}.\")\n",
    "    elif pred < last_known_price:\n",
    "        print(f\"The model suggests buying a PUT option for {ticker} expiring on {target_date}.\")\n",
    "    else:\n",
    "        print(f\"The model suggests no clear direction for {ticker} on {target_date}.\")\n",
    "    last_month = pd.Timestamp(target_date) - pd.DateOffset(months=1)\n",
    "    filtered_data = data[data['Date'] > last_month]\n",
    "    data_sorted_by_sentiment = filtered_data.sort_values(by='title_sentiment', key=abs, ascending=False)\n",
    "    top_10_news = data_sorted_by_sentiment[['Date', 'title', 'title_sentiment']].head(10)\n",
    "    print(\"\\nTop 10 influential\")\n",
    "# Fetch news for debugging purposes\n",
    "\n",
    "fetch_news('TSLA')\n",
    "# Call the main function\n",
    "\n",
    "execute('TSLA', '2025-2-26', sentiment_scaling_factor=10000.0)\n",
    "#%%\n",
    "#   BERT SENTIMENT MODEL\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from newsapi import NewsApiClient\n",
    "from transformers import pipeline\n",
    "# Initialize sentiment analysis using BERT from transformers\n",
    "\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
    "newsapi = NewsApiClient(api_key='ec064ce719114fe78bd3affdd71e5db8')  \n",
    "def fetch_data(ticker):\n",
    "    # Fetch 20 years of historical data for the given ticker using yfinance\n",
    "    t = yf.Ticker(ticker)\n",
    "    historical_data = t.history(period=\"20y\")\n",
    "    historical_data.reset_index(inplace=True)\n",
    "    # Remove timezone information to make it consistent with other datetime objects\n",
    "    historical_data['Date'] = historical_data['Date'].dt.tz_localize(None)\n",
    "    return historical_data\n",
    "\n",
    "def fetch_news(ticker):\n",
    "    # Fetch news articles for the given ticker for the past month\n",
    "    all_articles = newsapi.get_everything(q=f\"{ticker}\",\n",
    "    from_param='2024-12-07',\n",
    "    to='2024-12-30',\n",
    "    language='en',\n",
    "    sort_by='relevancy')\n",
    "    news_data = all_articles.get('articles', [])\n",
    "    if not news_data:\n",
    "        print(f\"No articles found for ticker: {ticker}\")\n",
    "        return pd.DataFrame(columns=['Date', 'title'])\n",
    "    news_df = pd.DataFrame(news_data)\n",
    "    # Remove timezone information to make it consistent with other datetime objects\n",
    "    news_df['Date'] = pd.to_datetime(news_df['publishedAt']).dt.tz_localize(None)\n",
    "    return news_df[['Date', 'title', 'description', 'content']]\n",
    "\n",
    "def extract_sentiment(news_df, sentiment_weight=1000.0):\n",
    "    # Combine title, description, and content for better sentiment analysis\n",
    "    news_df['combined_text'] = news_df['title'] + ' ' + news_df.get('description', '') + ' ' + news_df.get('content', '')\n",
    "    # Use BERT sentiment analysis to extract sentiment from the combined text\n",
    "    news_df['title_sentiment'] = news_df['combined_text'].apply(lambda x: sentiment_weight if sentiment_analysis(x)[0]['label'] == 'POSITIVE' else (-sentiment_weight if sentiment_analysis(x)[0]['label'] == 'NEGATIVE' else 0))\n",
    "    return news_df[['Date', 'title', 'title_sentiment', 'combined_text']]\n",
    "\n",
    "def merge_news_with_data(data, news):\n",
    "    # Merge historical stock data with news data on Date\n",
    "    merged_data = data.merge(news, on='Date', how='left')\n",
    "    merged_data.fillna(0, inplace=True)\n",
    "    # Calculate a weighted sentiment score based on sentiment and volume\n",
    "    merged_data['weighted_sentiment'] = merged_data['title_sentiment'] * merged_data['Volume']\n",
    "    return merged_data\n",
    "\n",
    "def process_data(data, look_back=250):\n",
    "    # Create a list to hold all the lag features\n",
    "    lag_columns = []\n",
    "    # Generate lag features and append them to the list\n",
    "    for i in range(1, look_back + 1):\n",
    "        lag_column = data['Close'].shift(i)\n",
    "        lag_column.name = f\"lag_{i}\"\n",
    "        lag_columns.append(lag_column)\n",
    "    # Combine all lag features into a single DataFrame\n",
    "    lag_features = pd.concat(lag_columns, axis=1)\n",
    "    # Combine the original data with the lag features\n",
    "    data = pd.concat([data, lag_features], axis=1)\n",
    "    data.dropna(inplace=True)\n",
    "    # Define columns that are not features for the model\n",
    "    columns_to_drop = ['Close', 'Date', 'title', 'combined_text']\n",
    "    # Split data into features and target variable\n",
    "    X = data.drop(columns_to_drop, axis=1)\n",
    "    y = data['Close']\n",
    "    # Normalize feature data and target data\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    X_scaled = scaler_x.fit_transform(X)\n",
    "    y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n",
    "    return pd.DataFrame(X_scaled, columns=X.columns), pd.Series(y_scaled.ravel()), scaler_x, scaler_y\n",
    "\n",
    "def execute(ticker, target_date, sentiment_weight=100000.0):\n",
    "    # Function to execute the entire pipeline\n",
    "    data = fetch_data(ticker)\n",
    "    news = fetch_news(ticker)\n",
    "    news = extract_sentiment(news, sentiment_weight)\n",
    "    data = merge_news_with_data(data, news)\n",
    "    X, y, scaler_x, scaler_y = process_data(data)\n",
    "    # Train a random forest regressor with the processed data\n",
    "    column_names = X.columns.tolist()\n",
    "    model = RandomForestRegressor(n_estimators=100)\n",
    "    model.fit(X, y)\n",
    "    # Predict the price for the target date\n",
    "    target_data = X.iloc[-1].values.reshape(1, -1)\n",
    "    pred_scaled = model.predict(target_data)\n",
    "    pred = scaler_y.inverse_transform([[pred_scaled[0]]])[0, 0]\n",
    "    print(f\"\\nThe predicted closing price for {ticker} on {target_date} is {pred}\")\n",
    "    # Provide a trading suggestion based on the prediction\n",
    "    last_known_price = y.iloc[-1]\n",
    "    if pred > last_known_price:\n",
    "        print(f\"The model suggests buying a CALL option for {ticker} expiring on {target_date}.\")\n",
    "    elif pred < last_known_price:\n",
    "        print(f\"The model suggests buying a PUT option for {ticker} expiring on {target_date}.\")\n",
    "    else:\n",
    "        print(f\"The model suggests no clear direction for {ticker} on {target_date}.\")\n",
    "    # Display importance of sentiment in the model\n",
    "    importance = model.feature_importances_\n",
    "    if 'title_sentiment' in column_names:\n",
    "        sentiment_index = column_names.index('title_sentiment')\n",
    "        sentiment_importance = importance[sentiment_index]\n",
    "        print(f\"\\nImportance of title_sentiment: {sentiment_importance}\")\n",
    "    # Display top 3 positive news articles\n",
    "    positive_news = news[news['title_sentiment'] == sentiment_weight].sort_values(by=\"Date\", ascending=False).head(3)\n",
    "    print(\"\\nTop 3 Positive News Articles (with combined text):\")\n",
    "    for idx, row in positive_news.iterrows():\n",
    "        print(\"\\nTitle:\", row['title'])\n",
    "        print(\"Combined Text:\", row['combined_text'])\n",
    "# Call the function with sentiment_weight\n",
    "\n",
    "execute('TSLA', '2025-12-26', sentiment_weight=100000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing NewsAPI key! Set it using os.environ.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py\u001b[0m in \u001b[0;36mline 3\n\u001b[1;32m      <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=261'>262</a>\u001b[0m NEWS_API_KEY \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mgetenv(\u001b[39m\"\u001b[39m\u001b[39mNEWS_API_KEY\u001b[39m\u001b[39m\"\u001b[39m)  \u001b[39m# Set your API key in environment variables\u001b[39;00m\n\u001b[1;32m      <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=262'>263</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m NEWS_API_KEY:\n\u001b[0;32m----> <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=263'>264</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mMissing NewsAPI key! Set it using os.environ.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=264'>265</a>\u001b[0m \u001b[39m# Initialize NewsAPI client\u001b[39;00m\n\u001b[1;32m      <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=266'>267</a>\u001b[0m newsapi \u001b[39m=\u001b[39m NewsApiClient(api_key\u001b[39m=\u001b[39mNEWS_API_KEY)\n",
      "\u001b[0;31mValueError\u001b[0m: Missing NewsAPI key! Set it using os.environ."
     ]
    }
   ],
   "source": [
    "NEWS_API_KEY = os.getenv(\"NEWS_API_KEY\")  # Set your API key in environment variables\n",
    "if not NEWS_API_KEY:\n",
    "    raise ValueError(\"Missing NewsAPI key! Set it using os.environ.\")\n",
    "# Initialize NewsAPI client\n",
    "\n",
    "newsapi = NewsApiClient(api_key=NEWS_API_KEY)\n",
    "# Initialize sentiment analysis using BERT\n",
    "\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
    "def fetch_data(ticker):\n",
    "    \"\"\"Fetch 20 years of historical stock data using yfinance.\"\"\"\n",
    "    t = yf.Ticker(ticker)\n",
    "    historical_data = t.history(period=\"20y\")\n",
    "    historical_data.reset_index(inplace=True)\n",
    "    historical_data['Date'] = historical_data['Date'].dt.tz_localize(None)\n",
    "    return historical_data\n",
    "\n",
    "def fetch_news(ticker):\n",
    "    \"\"\"Fetch recent news articles for the given stock ticker.\"\"\"\n",
    "    today = datetime.today().date()\n",
    "    # Set the max allowed date (from the NewsAPI error message)\n",
    "    max_allowed_date = datetime(2024, 12, 30).date()\n",
    "    # Adjust date range dynamically to avoid API errors\n",
    "    from_date = max(max_allowed_date, today - timedelta(days=7))\n",
    "    to_date = today\n",
    "    print(f\"Fetching news from {from_date} to {to_date}...\")\n",
    "    all_articles = newsapi.get_everything(\n",
    "        q=ticker,\n",
    "        from_param=from_date.strftime('%Y-%m-%d'),\n",
    "        to=to_date.strftime('%Y-%m-%d'),\n",
    "        language='en',\n",
    "        sort_by='relevancy'\n",
    "    )\n",
    "    news_data = all_articles.get('articles', [])\n",
    "    if not news_data:\n",
    "        print(f\"No articles found for {ticker}\")\n",
    "        return pd.DataFrame(columns=['Date', 'title', 'description', 'content'])\n",
    "    news_df = pd.DataFrame(news_data)\n",
    "    news_df['Date'] = pd.to_datetime(news_df['publishedAt']).dt.tz_localize(None)\n",
    "    return news_df[['Date', 'title', 'description', 'content']]\n",
    "\n",
    "def extract_sentiment(news_df):\n",
    "    \"\"\"Extract sentiment scores using BERT sentiment analysis.\"\"\"\n",
    "    if news_df.empty:\n",
    "        return news_df\n",
    "    news_df['combined_text'] = news_df['title'].fillna('') + ' ' + news_df['description'].fillna('') + ' ' + news_df['content'].fillna('')\n",
    "    news_df['sentiment_score'] = news_df['combined_text'].apply(lambda x: sentiment_analysis(x[:512])[0]['score'])  # Limit to 512 characters\n",
    "    news_df['sentiment_label'] = news_df['combined_text'].apply(lambda x: sentiment_analysis(x[:512])[0]['label'])\n",
    "    # Convert labels into numerical values\n",
    "    news_df['title_sentiment'] = news_df.apply(\n",
    "        lambda x: x['sentiment_score'] if x['sentiment_label'] == 'POSITIVE' else \n",
    "                  (-x['sentiment_score'] if x['sentiment_label'] == 'NEGATIVE' else 0),\n",
    "        axis=1\n",
    "    )\n",
    "    return news_df[['Date', 'title', 'title_sentiment', 'combined_text']]\n",
    "\n",
    "def merge_news_with_data(data, news):\n",
    "    \"\"\"Merge stock data with news sentiment scores.\"\"\"\n",
    "    if news.empty:\n",
    "        data['title_sentiment'] = 0\n",
    "        return data\n",
    "    merged_data = data.merge(news, on='Date', how='left')\n",
    "    merged_data.fillna({'title_sentiment': 0}, inplace=True)\n",
    "    # Weight sentiment by volume for impact\n",
    "    merged_data['weighted_sentiment'] = merged_data['title_sentiment'] * merged_data['Volume']\n",
    "    return merged_data\n",
    "\n",
    "def process_data(data, look_back=60):\n",
    "    \"\"\"Create lag features and prepare data for model training.\"\"\"\n",
    "    for i in range(1, look_back + 1):\n",
    "        data[f\"lag_{i}\"] = data['Close'].shift(i)\n",
    "    data.dropna(inplace=True)\n",
    "    # Define columns to exclude from training\n",
    "    columns_to_drop = ['Close', 'Date', 'title', 'combined_text']\n",
    "    X = data.drop(columns=[col for col in columns_to_drop if col in data.columns])\n",
    "    y = data['Close']\n",
    "    # Scale data\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    X_scaled = scaler_x.fit_transform(X)\n",
    "    y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n",
    "    return pd.DataFrame(X_scaled, columns=X.columns), pd.Series(y_scaled.ravel()), scaler_x, scaler_y\n",
    "\n",
    "def plot_feature_importance(model, X):\n",
    "    \"\"\"Plot feature importance of the trained model.\"\"\"\n",
    "    importance = model.feature_importances_\n",
    "    features = X.columns\n",
    "    sorted_idx = np.argsort(importance)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh([features[i] for i in sorted_idx[-10:]], importance[sorted_idx[-10:]])\n",
    "    plt.xlabel(\"Feature Importance\")\n",
    "    plt.title(\"Top 10 Important Features\")\n",
    "    plt.show()\n",
    "\n",
    "def execute(ticker, target_date):\n",
    "    \"\"\"Execute the stock prediction pipeline.\"\"\"\n",
    "    look_back = 60\n",
    "    # Fetch and process stock data\n",
    "    data = fetch_data(ticker)\n",
    "    # Fetch and process news sentiment\n",
    "    news = fetch_news(ticker)\n",
    "    news = extract_sentiment(news)\n",
    "    data = merge_news_with_data(data, news)\n",
    "    # Prepare data for model training\n",
    "    X, y, scaler_x, scaler_y = process_data(data, look_back)\n",
    "    # Train model\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X, y)\n",
    "    # Make prediction\n",
    "    target_data = X.iloc[-1].values.reshape(1, -1)\n",
    "    pred_scaled = model.predict(target_data)\n",
    "    pred = scaler_y.inverse_transform([[pred_scaled[0]]])[0, 0]\n",
    "    print(f\"\\nüìà Predicted closing price for {ticker} on {target_date}: **${pred:.2f}**\")\n",
    "    # Generate trading recommendation\n",
    "    last_known_price = scaler_y.inverse_transform([[y.iloc[-1]]])[0, 0]\n",
    "    if pred > last_known_price:\n",
    "        print(f\"üìä **Recommendation:** Buy a **CALL** option for {ticker} expiring on {target_date}.\")\n",
    "    elif pred < last_known_price:\n",
    "        print(f\"üìâ **Recommendation:** Buy a **PUT** option for {ticker} expiring on {target_date}.\")\n",
    "    else:\n",
    "        print(f\"‚öñÔ∏è No clear direction for {ticker} on {target_date}.\")\n",
    "    # Plot feature importance\n",
    "    plot_feature_importance(model, X)\n",
    "    # Display top 3 positive news articles\n",
    "    positive_news = news[news['title_sentiment'] > 0].sort_values(by=\"Date\", ascending=False).head(3)\n",
    "    print(\"\\nüîç **Top 3 Positive News Articles:**\")\n",
    "    for _, row in positive_news.iterrows():\n",
    "        print(f\"\\nüìå **Title:** {row['title']}\")\n",
    "        print(f\"üìù **Content:** {row['combined_text'][:200]}...\")  # Display first 200 characters\n",
    "# Run the model\n",
    "\n",
    "execute('TSLA', '2025-12-26')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching news from 2025-01-24 to 2025-01-31...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 68)) while a minimum of 1 is required by MinMaxScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py\u001b[0m in \u001b[0;36mline 140\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=385'>386</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39müìù **Content:** \u001b[39m\u001b[39m{\u001b[39;00mrow[\u001b[39m'\u001b[39m\u001b[39mcombined_text\u001b[39m\u001b[39m'\u001b[39m][:\u001b[39m200\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)  \u001b[39m# Display first 200 characters\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=386'>387</a>\u001b[0m \u001b[39m# Run the model\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=388'>389</a>\u001b[0m execute(\u001b[39m'\u001b[39m\u001b[39mTSLA\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m2025-12-26\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py\u001b[0m in \u001b[0;36mline 113\u001b[0m, in \u001b[0;36mexecute\u001b[0;34m(ticker, target_date)\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=359'>360</a>\u001b[0m data \u001b[39m=\u001b[39m merge_news_with_data(data, news)\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=360'>361</a>\u001b[0m \u001b[39m# Prepare data for model training\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=361'>362</a>\u001b[0m X, y, scaler_x, scaler_y \u001b[39m=\u001b[39m process_data(data, look_back)\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=362'>363</a>\u001b[0m \u001b[39m# Train model\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=363'>364</a>\u001b[0m model \u001b[39m=\u001b[39m RandomForestRegressor(n_estimators\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
      "\u001b[1;32m/Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py\u001b[0m in \u001b[0;36mline 88\u001b[0m, in \u001b[0;36mprocess_data\u001b[0;34m(data, look_back)\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=334'>335</a>\u001b[0m scaler_x \u001b[39m=\u001b[39m MinMaxScaler()\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=335'>336</a>\u001b[0m scaler_y \u001b[39m=\u001b[39m MinMaxScaler()\n\u001b[0;32m---> <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=336'>337</a>\u001b[0m X_scaled \u001b[39m=\u001b[39m scaler_x\u001b[39m.\u001b[39mfit_transform(X)\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=337'>338</a>\u001b[0m y_scaled \u001b[39m=\u001b[39m scaler_y\u001b[39m.\u001b[39mfit_transform(y\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=338'>339</a>\u001b[0m \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39mDataFrame(X_scaled, columns\u001b[39m=\u001b[39mX\u001b[39m.\u001b[39mcolumns), pd\u001b[39m.\u001b[39mSeries(y_scaled\u001b[39m.\u001b[39mravel()), scaler_x, scaler_y\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py?line=137'>138</a>\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py?line=138'>139</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py?line=139'>140</a>\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py?line=140'>141</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py?line=141'>142</a>\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py?line=142'>143</a>\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py?line=143'>144</a>\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py?line=144'>145</a>\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py?line=145'>146</a>\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/base.py:915\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=910'>911</a>\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=911'>912</a>\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=912'>913</a>\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=913'>914</a>\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=914'>915</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=915'>916</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=916'>917</a>\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=917'>918</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:434\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py?line=431'>432</a>\u001b[0m \u001b[39m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py?line=432'>433</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py?line=433'>434</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartial_fit(X, y)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=1143'>1144</a>\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=1145'>1146</a>\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=1146'>1147</a>\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=1147'>1148</a>\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=1148'>1149</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=1149'>1150</a>\u001b[0m ):\n\u001b[0;32m-> <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=1150'>1151</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:472\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py?line=465'>466</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py?line=466'>467</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMinMaxScaler does not support sparse input. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py?line=467'>468</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConsider using MaxAbsScaler instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py?line=468'>469</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py?line=470'>471</a>\u001b[0m first_pass \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mn_samples_seen_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py?line=471'>472</a>\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py?line=472'>473</a>\u001b[0m     X,\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py?line=473'>474</a>\u001b[0m     reset\u001b[39m=\u001b[39mfirst_pass,\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py?line=474'>475</a>\u001b[0m     dtype\u001b[39m=\u001b[39mFLOAT_DTYPES,\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py?line=475'>476</a>\u001b[0m     force_all_finite\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mallow-nan\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py?line=476'>477</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py?line=478'>479</a>\u001b[0m data_min \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnanmin(X, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py?line=479'>480</a>\u001b[0m data_max \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnanmax(X, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=601'>602</a>\u001b[0m         out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=602'>603</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=603'>604</a>\u001b[0m     out \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=604'>605</a>\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=605'>606</a>\u001b[0m     out \u001b[39m=\u001b[39m _check_y(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:969\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/validation.py?line=966'>967</a>\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/validation.py?line=967'>968</a>\u001b[0m     \u001b[39mif\u001b[39;00m n_samples \u001b[39m<\u001b[39m ensure_min_samples:\n\u001b[0;32m--> <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/validation.py?line=968'>969</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/validation.py?line=969'>970</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m sample(s) (shape=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) while a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/validation.py?line=970'>971</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m minimum of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m is required\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/validation.py?line=971'>972</a>\u001b[0m             \u001b[39m%\u001b[39m (n_samples, array\u001b[39m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/validation.py?line=972'>973</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/validation.py?line=974'>975</a>\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_features \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/validation.py?line=975'>976</a>\u001b[0m     n_features \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 68)) while a minimum of 1 is required by MinMaxScaler."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from newsapi import NewsApiClient\n",
    "from transformers import pipeline\n",
    "NEWS_API_KEY = \"ec064ce719114fe78bd3affdd71e5db8\"  # Replace with your actual API key\n",
    "# Initialize NewsAPI client\n",
    "\n",
    "newsapi = NewsApiClient(api_key=NEWS_API_KEY)\n",
    "# Initialize sentiment analysis using BERT\n",
    "\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
    "def fetch_data(ticker):\n",
    "    \"\"\"Fetch 20 years of historical stock data using yfinance.\"\"\"\n",
    "    t = yf.Ticker(ticker)\n",
    "    historical_data = t.history(period=\"20y\")\n",
    "    historical_data.reset_index(inplace=True)\n",
    "    historical_data['Date'] = historical_data['Date'].dt.tz_localize(None)\n",
    "    return historical_data\n",
    "\n",
    "def fetch_news(ticker):\n",
    "    \"\"\"Fetch recent news articles for the given stock ticker.\"\"\"\n",
    "    today = datetime.today().date()\n",
    "    # Set the max allowed date (from the NewsAPI error message)\n",
    "    max_allowed_date = datetime(2024, 12, 30).date()\n",
    "    # Adjust date range dynamically to avoid API errors\n",
    "    from_date = max(max_allowed_date, today - timedelta(days=7))\n",
    "    to_date = today\n",
    "    print(f\"Fetching news from {from_date} to {to_date}...\")\n",
    "    all_articles = newsapi.get_everything(\n",
    "        q=ticker,\n",
    "        from_param=from_date.strftime('%Y-%m-%d'),\n",
    "        to=to_date.strftime('%Y-%m-%d'),\n",
    "        language='en',\n",
    "        sort_by='relevancy'\n",
    "    )\n",
    "    news_data = all_articles.get('articles', [])\n",
    "    if not news_data:\n",
    "        print(f\"No articles found for {ticker}\")\n",
    "        return pd.DataFrame(columns=['Date', 'title', 'description', 'content'])\n",
    "    news_df = pd.DataFrame(news_data)\n",
    "    news_df['Date'] = pd.to_datetime(news_df['publishedAt']).dt.tz_localize(None)\n",
    "    return news_df[['Date', 'title', 'description', 'content']]\n",
    "\n",
    "def extract_sentiment(news_df):\n",
    "    \"\"\"Extract sentiment scores using BERT sentiment analysis.\"\"\"\n",
    "    if news_df.empty:\n",
    "        return news_df\n",
    "    news_df['combined_text'] = news_df['title'].fillna('') + ' ' + news_df['description'].fillna('') + ' ' + news_df['content'].fillna('')\n",
    "    news_df['sentiment_score'] = news_df['combined_text'].apply(lambda x: sentiment_analysis(x[:512])[0]['score'])  # Limit to 512 characters\n",
    "    news_df['sentiment_label'] = news_df['combined_text'].apply(lambda x: sentiment_analysis(x[:512])[0]['label'])\n",
    "    # Convert labels into numerical values\n",
    "    news_df['title_sentiment'] = news_df.apply(\n",
    "        lambda x: x['sentiment_score'] if x['sentiment_label'] == 'POSITIVE' else \n",
    "                  (-x['sentiment_score'] if x['sentiment_label'] == 'NEGATIVE' else 0),\n",
    "        axis=1\n",
    "    )\n",
    "    return news_df[['Date', 'title', 'title_sentiment', 'combined_text']]\n",
    "\n",
    "def merge_news_with_data(data, news):\n",
    "    \"\"\"Merge stock data with news sentiment scores.\"\"\"\n",
    "    if news.empty:\n",
    "        data['title_sentiment'] = 0\n",
    "        return data\n",
    "    merged_data = data.merge(news, on='Date', how='left')\n",
    "    merged_data.fillna({'title_sentiment': 0}, inplace=True)\n",
    "    # Weight sentiment by volume for impact\n",
    "    merged_data['weighted_sentiment'] = merged_data['title_sentiment'] * merged_data['Volume']\n",
    "    return merged_data\n",
    "\n",
    "def process_data(data, look_back=60):\n",
    "    \"\"\"Create lag features and prepare data for model training.\"\"\"\n",
    "    for i in range(1, look_back + 1):\n",
    "        data[f\"lag_{i}\"] = data['Close'].shift(i)\n",
    "    data.dropna(inplace=True)\n",
    "    # Define columns to exclude from training\n",
    "    columns_to_drop = ['Close', 'Date', 'title', 'combined_text']\n",
    "    X = data.drop(columns=[col for col in columns_to_drop if col in data.columns])\n",
    "    y = data['Close']\n",
    "    # Scale data\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    X_scaled = scaler_x.fit_transform(X)\n",
    "    y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n",
    "    return pd.DataFrame(X_scaled, columns=X.columns), pd.Series(y_scaled.ravel()), scaler_x, scaler_y\n",
    "\n",
    "def plot_feature_importance(model, X):\n",
    "    \"\"\"Plot feature importance of the trained model.\"\"\"\n",
    "    importance = model.feature_importances_\n",
    "    features = X.columns\n",
    "    sorted_idx = np.argsort(importance)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh([features[i] for i in sorted_idx[-10:]], importance[sorted_idx[-10:]])\n",
    "    plt.xlabel(\"Feature Importance\")\n",
    "    plt.title(\"Top 10 Important Features\")\n",
    "    plt.show()\n",
    "\n",
    "def execute(ticker, target_date):\n",
    "    \"\"\"Execute the stock prediction pipeline.\"\"\"\n",
    "    look_back = 60\n",
    "    # Fetch and process stock data\n",
    "    data = fetch_data(ticker)\n",
    "    # Fetch and process news sentiment\n",
    "    news = fetch_news(ticker)\n",
    "    news = extract_sentiment(news)\n",
    "    data = merge_news_with_data(data, news)\n",
    "    # Prepare data for model training\n",
    "    X, y, scaler_x, scaler_y = process_data(data, look_back)\n",
    "    # Train model\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X, y)\n",
    "    # Make prediction\n",
    "    target_data = X.iloc[-1].values.reshape(1, -1)\n",
    "    pred_scaled = model.predict(target_data)\n",
    "    pred = scaler_y.inverse_transform([[pred_scaled[0]]])[0, 0]\n",
    "    print(f\"\\nüìà Predicted closing price for {ticker} on {target_date}: **${pred:.2f}**\")\n",
    "    # Generate trading recommendation\n",
    "    last_known_price = scaler_y.inverse_transform([[y.iloc[-1]]])[0, 0]\n",
    "    if pred > last_known_price:\n",
    "        print(f\"üìä **Recommendation:** Buy a **CALL** option for {ticker} expiring on {target_date}.\")\n",
    "    elif pred < last_known_price:\n",
    "        print(f\"üìâ **Recommendation:** Buy a **PUT** option for {ticker} expiring on {target_date}.\")\n",
    "    else:\n",
    "        print(f\"‚öñÔ∏è No clear direction for {ticker} on {target_date}.\")\n",
    "    # Plot feature importance\n",
    "    plot_feature_importance(model, X)\n",
    "    # Display top 3 positive news articles\n",
    "    positive_news = news[news['title_sentiment'] > 0].sort_values(by=\"Date\", ascending=False).head(3)\n",
    "    print(\"\\nüîç **Top 3 Positive News Articles:**\")\n",
    "    for _, row in positive_news.iterrows():\n",
    "        print(f\"\\nüìå **Title:** {row['title']}\")\n",
    "        print(f\"üìù **Content:** {row['combined_text'][:200]}...\")  # Display first 200 characters\n",
    "# Run the model\n",
    "\n",
    "execute('TSLA', '2025-12-26')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching news from 2025-01-24 to 2025-01-31...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 68)) while a minimum of 1 is required by MinMaxScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py\u001b[0m in \u001b[0;36mline 140\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=385'>386</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m **Content:** \u001b[39m\u001b[39m{\u001b[39;00mrow[\u001b[39m'\u001b[39m\u001b[39mcombined_text\u001b[39m\u001b[39m'\u001b[39m][:\u001b[39m200\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)  \u001b[39m# Display first 200 characters\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=386'>387</a>\u001b[0m \u001b[39m# Run the model\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=388'>389</a>\u001b[0m execute(\u001b[39m'\u001b[39m\u001b[39mTSLA\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m2025-12-26\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py\u001b[0m in \u001b[0;36mline 113\u001b[0m, in \u001b[0;36mexecute\u001b[0;34m(ticker, target_date)\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=359'>360</a>\u001b[0m data \u001b[39m=\u001b[39m merge_news_with_data(data, news)\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=360'>361</a>\u001b[0m \u001b[39m# Prepare data for model training\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=361'>362</a>\u001b[0m X, y, scaler_x, scaler_y \u001b[39m=\u001b[39m process_data(data, look_back)\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=362'>363</a>\u001b[0m \u001b[39m# Train model\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=363'>364</a>\u001b[0m model \u001b[39m=\u001b[39m RandomForestRegressor(n_estimators\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
      "\u001b[1;32m/Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py\u001b[0m in \u001b[0;36mline 88\u001b[0m, in \u001b[0;36mprocess_data\u001b[0;34m(data, look_back)\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=334'>335</a>\u001b[0m scaler_x \u001b[39m=\u001b[39m MinMaxScaler()\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=335'>336</a>\u001b[0m scaler_y \u001b[39m=\u001b[39m MinMaxScaler()\n\u001b[0;32m---> <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=336'>337</a>\u001b[0m X_scaled \u001b[39m=\u001b[39m scaler_x\u001b[39m.\u001b[39mfit_transform(X)\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=337'>338</a>\u001b[0m y_scaled \u001b[39m=\u001b[39m scaler_y\u001b[39m.\u001b[39mfit_transform(y\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=338'>339</a>\u001b[0m \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39mDataFrame(X_scaled, columns\u001b[39m=\u001b[39mX\u001b[39m.\u001b[39mcolumns), pd\u001b[39m.\u001b[39mSeries(y_scaled\u001b[39m.\u001b[39mravel()), scaler_x, scaler_y\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py?line=137'>138</a>\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py?line=138'>139</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py?line=139'>140</a>\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py?line=140'>141</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py?line=141'>142</a>\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py?line=142'>143</a>\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py?line=143'>144</a>\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py?line=144'>145</a>\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py?line=145'>146</a>\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/base.py:915\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=910'>911</a>\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=911'>912</a>\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=912'>913</a>\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=913'>914</a>\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=914'>915</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=915'>916</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=916'>917</a>\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=917'>918</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:434\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py?line=431'>432</a>\u001b[0m \u001b[39m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py?line=432'>433</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py?line=433'>434</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartial_fit(X, y)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=1143'>1144</a>\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=1145'>1146</a>\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=1146'>1147</a>\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=1147'>1148</a>\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=1148'>1149</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=1149'>1150</a>\u001b[0m ):\n\u001b[0;32m-> <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=1150'>1151</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:472\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py?line=465'>466</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py?line=466'>467</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMinMaxScaler does not support sparse input. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py?line=467'>468</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConsider using MaxAbsScaler instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py?line=468'>469</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py?line=470'>471</a>\u001b[0m first_pass \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mn_samples_seen_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py?line=471'>472</a>\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py?line=472'>473</a>\u001b[0m     X,\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py?line=473'>474</a>\u001b[0m     reset\u001b[39m=\u001b[39mfirst_pass,\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py?line=474'>475</a>\u001b[0m     dtype\u001b[39m=\u001b[39mFLOAT_DTYPES,\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py?line=475'>476</a>\u001b[0m     force_all_finite\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mallow-nan\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py?line=476'>477</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py?line=478'>479</a>\u001b[0m data_min \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnanmin(X, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py?line=479'>480</a>\u001b[0m data_max \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnanmax(X, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=601'>602</a>\u001b[0m         out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=602'>603</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=603'>604</a>\u001b[0m     out \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=604'>605</a>\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/base.py?line=605'>606</a>\u001b[0m     out \u001b[39m=\u001b[39m _check_y(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:969\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/validation.py?line=966'>967</a>\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/validation.py?line=967'>968</a>\u001b[0m     \u001b[39mif\u001b[39;00m n_samples \u001b[39m<\u001b[39m ensure_min_samples:\n\u001b[0;32m--> <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/validation.py?line=968'>969</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/validation.py?line=969'>970</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m sample(s) (shape=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) while a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/validation.py?line=970'>971</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m minimum of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m is required\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/validation.py?line=971'>972</a>\u001b[0m             \u001b[39m%\u001b[39m (n_samples, array\u001b[39m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/validation.py?line=972'>973</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/validation.py?line=974'>975</a>\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_features \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/sklearn/utils/validation.py?line=975'>976</a>\u001b[0m     n_features \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 68)) while a minimum of 1 is required by MinMaxScaler."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from newsapi import NewsApiClient\n",
    "from transformers import pipeline\n",
    "NEWS_API_KEY = \"ec064ce719114fe78bd3affdd71e5db8\"  # Replace with your actual API key\n",
    "# Initialize NewsAPI client\n",
    "\n",
    "newsapi = NewsApiClient(api_key=NEWS_API_KEY)\n",
    "# Initialize sentiment analysis using BERT\n",
    "\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
    "def fetch_data(ticker):\n",
    "    \"\"\"Fetch 20 years of historical stock data using yfinance.\"\"\"\n",
    "    t = yf.Ticker(ticker)\n",
    "    historical_data = t.history(period=\"20y\")\n",
    "    historical_data.reset_index(inplace=True)\n",
    "    historical_data['Date'] = historical_data['Date'].dt.tz_localize(None)\n",
    "    return historical_data\n",
    "\n",
    "def fetch_news(ticker):\n",
    "    \"\"\"Fetch recent news articles for the given stock ticker.\"\"\"\n",
    "    today = datetime.today().date()\n",
    "    # Set the max allowed date (from the NewsAPI error message)\n",
    "    max_allowed_date = datetime(2024, 12, 30).date()\n",
    "    # Adjust date range dynamically to avoid API errors\n",
    "    from_date = max(max_allowed_date, today - timedelta(days=7))\n",
    "    to_date = today\n",
    "    print(f\"Fetching news from {from_date} to {to_date}...\")\n",
    "    all_articles = newsapi.get_everything(\n",
    "        q=ticker,\n",
    "        from_param=from_date.strftime('%Y-%m-%d'),\n",
    "        to=to_date.strftime('%Y-%m-%d'),\n",
    "        language='en',\n",
    "        sort_by='relevancy'\n",
    "    )\n",
    "    news_data = all_articles.get('articles', [])\n",
    "    if not news_data:\n",
    "        print(f\"No articles found for {ticker}\")\n",
    "        return pd.DataFrame(columns=['Date', 'title', 'description', 'content'])\n",
    "    news_df = pd.DataFrame(news_data)\n",
    "    news_df['Date'] = pd.to_datetime(news_df['publishedAt']).dt.tz_localize(None)\n",
    "    return news_df[['Date', 'title', 'description', 'content']]\n",
    "\n",
    "def extract_sentiment(news_df):\n",
    "    \"\"\"Extract sentiment scores using BERT sentiment analysis.\"\"\"\n",
    "    if news_df.empty:\n",
    "        return news_df\n",
    "    news_df['combined_text'] = news_df['title'].fillna('') + ' ' + news_df['description'].fillna('') + ' ' + news_df['content'].fillna('')\n",
    "    news_df['sentiment_score'] = news_df['combined_text'].apply(lambda x: sentiment_analysis(x[:512])[0]['score'])  # Limit to 512 characters\n",
    "    news_df['sentiment_label'] = news_df['combined_text'].apply(lambda x: sentiment_analysis(x[:512])[0]['label'])\n",
    "    # Convert labels into numerical values\n",
    "    news_df['title_sentiment'] = news_df.apply(\n",
    "        lambda x: x['sentiment_score'] if x['sentiment_label'] == 'POSITIVE' else \n",
    "                  (-x['sentiment_score'] if x['sentiment_label'] == 'NEGATIVE' else 0),\n",
    "        axis=1\n",
    "    )\n",
    "    return news_df[['Date', 'title', 'title_sentiment', 'combined_text']]\n",
    "\n",
    "def merge_news_with_data(data, news):\n",
    "    \"\"\"Merge stock data with news sentiment scores.\"\"\"\n",
    "    if news.empty:\n",
    "        data['title_sentiment'] = 0\n",
    "        return data\n",
    "    merged_data = data.merge(news, on='Date', how='left')\n",
    "    merged_data.fillna({'title_sentiment': 0}, inplace=True)\n",
    "    # Weight sentiment by volume for impact\n",
    "    merged_data['weighted_sentiment'] = merged_data['title_sentiment'] * merged_data['Volume']\n",
    "    return merged_data\n",
    "\n",
    "def process_data(data, look_back=60):\n",
    "    \"\"\"Create lag features and prepare data for model training.\"\"\"\n",
    "    for i in range(1, look_back + 1):\n",
    "        data[f\"lag_{i}\"] = data['Close'].shift(i)\n",
    "    data.dropna(inplace=True)\n",
    "    # Define columns to exclude from training\n",
    "    columns_to_drop = ['Close', 'Date', 'title', 'combined_text']\n",
    "    X = data.drop(columns=[col for col in columns_to_drop if col in data.columns])\n",
    "    y = data['Close']\n",
    "    # Scale data\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    X_scaled = scaler_x.fit_transform(X)\n",
    "    y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n",
    "    return pd.DataFrame(X_scaled, columns=X.columns), pd.Series(y_scaled.ravel()), scaler_x, scaler_y\n",
    "\n",
    "def plot_feature_importance(model, X):\n",
    "    \"\"\"Plot feature importance of the trained model.\"\"\"\n",
    "    importance = model.feature_importances_\n",
    "    features = X.columns\n",
    "    sorted_idx = np.argsort(importance)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh([features[i] for i in sorted_idx[-10:]], importance[sorted_idx[-10:]])\n",
    "    plt.xlabel(\"Feature Importance\")\n",
    "    plt.title(\"Top 10 Important Features\")\n",
    "    plt.show()\n",
    "\n",
    "def execute(ticker, target_date):\n",
    "    \"\"\"Execute the stock prediction pipeline.\"\"\"\n",
    "    look_back = 60\n",
    "    # Fetch and process stock data\n",
    "    data = fetch_data(ticker)\n",
    "    # Fetch and process news sentiment\n",
    "    news = fetch_news(ticker)\n",
    "    news = extract_sentiment(news)\n",
    "    data = merge_news_with_data(data, news)\n",
    "    # Prepare data for model training\n",
    "    X, y, scaler_x, scaler_y = process_data(data, look_back)\n",
    "    # Train model\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X, y)\n",
    "    # Make prediction\n",
    "    target_data = X.iloc[-1].values.reshape(1, -1)\n",
    "    pred_scaled = model.predict(target_data)\n",
    "    pred = scaler_y.inverse_transform([[pred_scaled[0]]])[0, 0]\n",
    "    print(f\"\\nüìà Predicted closing price for {ticker} on {target_date}: **${pred:.2f}**\")\n",
    "    # Generate trading recommendation\n",
    "    last_known_price = scaler_y.inverse_transform([[y.iloc[-1]]])[0, 0]\n",
    "    if pred > last_known_price:\n",
    "        print(f\"üìä **Recommendation:** Buy a **CALL** option for {ticker} expiring on {target_date}.\")\n",
    "    elif pred < last_known_price:\n",
    "        print(f\"üìâ **Recommendation:** Buy a **PUT** option for {ticker} expiring on {target_date}.\")\n",
    "    else:\n",
    "        print(f\"‚öñÔ∏è No clear direction for {ticker} on {target_date}.\")\n",
    "    # Plot feature importance\n",
    "    plot_feature_importance(model, X)\n",
    "    # Display top 3 positive news articles\n",
    "    positive_news = news[news['title_sentiment'] > 0].sort_values(by=\"Date\", ascending=False).head(3)\n",
    "    print(\"\\nüîç **Top 3 Positive News Articles:**\")\n",
    "    for _, row in positive_news.iterrows():\n",
    "        print(f\"\\n **Title:** {row['title']}\")\n",
    "        print(f\" **Content:** {row['combined_text'][:200]}...\")  # Display first 200 characters\n",
    "# Run the model\n",
    "\n",
    "execute('TSLA', '2025-12-26')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching news from 2025-01-24 to 2025-01-31...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "After applying lag features, no rows remain. Ensure the dataset has sufficient historical data.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py\u001b[0m in \u001b[0;36mline 149\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=394'>395</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m **Content:** \u001b[39m\u001b[39m{\u001b[39;00mrow[\u001b[39m'\u001b[39m\u001b[39mcombined_text\u001b[39m\u001b[39m'\u001b[39m][:\u001b[39m200\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)  \u001b[39m# Display first 200 characters\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=395'>396</a>\u001b[0m \u001b[39m# Run the model\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=397'>398</a>\u001b[0m execute(\u001b[39m'\u001b[39m\u001b[39mTSLA\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m2025-12-26\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py\u001b[0m in \u001b[0;36mline 122\u001b[0m, in \u001b[0;36mexecute\u001b[0;34m(ticker, target_date)\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=368'>369</a>\u001b[0m data \u001b[39m=\u001b[39m merge_news_with_data(data, news)\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=369'>370</a>\u001b[0m \u001b[39m# Prepare data for model training\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=370'>371</a>\u001b[0m X, y, scaler_x, scaler_y \u001b[39m=\u001b[39m process_data(data, look_back)\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=371'>372</a>\u001b[0m \u001b[39m# Train model\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=372'>373</a>\u001b[0m model \u001b[39m=\u001b[39m RandomForestRegressor(n_estimators\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
      "\u001b[1;32m/Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py\u001b[0m in \u001b[0;36mline 86\u001b[0m, in \u001b[0;36mprocess_data\u001b[0;34m(data, look_back)\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=332'>333</a>\u001b[0m \u001b[39m# Ensure that we still have data after dropping NaNs\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=333'>334</a>\u001b[0m \u001b[39mif\u001b[39;00m data\u001b[39m.\u001b[39mempty:\n\u001b[0;32m---> <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=334'>335</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAfter applying lag features, no rows remain. Ensure the dataset has sufficient historical data.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=335'>336</a>\u001b[0m \u001b[39m# Define columns to exclude from training\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=336'>337</a>\u001b[0m columns_to_drop \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mClose\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mDate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcombined_text\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: After applying lag features, no rows remain. Ensure the dataset has sufficient historical data."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from newsapi import NewsApiClient\n",
    "from transformers import pipeline\n",
    "NEWS_API_KEY = \"ec064ce719114fe78bd3affdd71e5db8\"  # Replace with your actual API key\n",
    "# Initialize NewsAPI client\n",
    "\n",
    "newsapi = NewsApiClient(api_key=NEWS_API_KEY)\n",
    "# Initialize sentiment analysis using BERT\n",
    "\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
    "def fetch_data(ticker):\n",
    "    \"\"\"Fetch 20 years of historical stock data using yfinance.\"\"\"\n",
    "    t = yf.Ticker(ticker)\n",
    "    historical_data = t.history(period=\"20y\")\n",
    "    historical_data.reset_index(inplace=True)\n",
    "    historical_data['Date'] = historical_data['Date'].dt.tz_localize(None)\n",
    "    return historical_data\n",
    "\n",
    "def fetch_news(ticker):\n",
    "    \"\"\"Fetch recent news articles for the given stock ticker.\"\"\"\n",
    "    today = datetime.today().date()\n",
    "    # Set the max allowed date (from the NewsAPI error message)\n",
    "    max_allowed_date = datetime(2024, 12, 30).date()\n",
    "    # Adjust date range dynamically to avoid API errors\n",
    "    from_date = max(max_allowed_date, today - timedelta(days=7))\n",
    "    to_date = today\n",
    "    print(f\"Fetching news from {from_date} to {to_date}...\")\n",
    "    all_articles = newsapi.get_everything(\n",
    "        q=ticker,\n",
    "        from_param=from_date.strftime('%Y-%m-%d'),\n",
    "        to=to_date.strftime('%Y-%m-%d'),\n",
    "        language='en',\n",
    "        sort_by='relevancy'\n",
    "    )\n",
    "    news_data = all_articles.get('articles', [])\n",
    "    if not news_data:\n",
    "        print(f\"No articles found for {ticker}\")\n",
    "        return pd.DataFrame(columns=['Date', 'title', 'description', 'content'])\n",
    "    news_df = pd.DataFrame(news_data)\n",
    "    news_df['Date'] = pd.to_datetime(news_df['publishedAt']).dt.tz_localize(None)\n",
    "    return news_df[['Date', 'title', 'description', 'content']]\n",
    "\n",
    "def extract_sentiment(news_df):\n",
    "    \"\"\"Extract sentiment scores using BERT sentiment analysis.\"\"\"\n",
    "    if news_df.empty:\n",
    "        return news_df\n",
    "    news_df['combined_text'] = news_df['title'].fillna('') + ' ' + news_df['description'].fillna('') + ' ' + news_df['content'].fillna('')\n",
    "    news_df['sentiment_score'] = news_df['combined_text'].apply(lambda x: sentiment_analysis(x[:512])[0]['score'])  # Limit to 512 characters\n",
    "    news_df['sentiment_label'] = news_df['combined_text'].apply(lambda x: sentiment_analysis(x[:512])[0]['label'])\n",
    "    # Convert labels into numerical values\n",
    "    news_df['title_sentiment'] = news_df.apply(\n",
    "        lambda x: x['sentiment_score'] if x['sentiment_label'] == 'POSITIVE' else \n",
    "                  (-x['sentiment_score'] if x['sentiment_label'] == 'NEGATIVE' else 0),\n",
    "        axis=1\n",
    "    )\n",
    "    return news_df[['Date', 'title', 'title_sentiment', 'combined_text']]\n",
    "\n",
    "def merge_news_with_data(data, news):\n",
    "    \"\"\"Merge stock data with news sentiment scores.\"\"\"\n",
    "    if news.empty:\n",
    "        data['title_sentiment'] = 0\n",
    "        return data\n",
    "    merged_data = data.merge(news, on='Date', how='left')\n",
    "    merged_data.fillna({'title_sentiment': 0}, inplace=True)\n",
    "    # Weight sentiment by volume for impact\n",
    "    merged_data['weighted_sentiment'] = merged_data['title_sentiment'] * merged_data['Volume']\n",
    "    return merged_data\n",
    "\n",
    "def process_data(data, look_back=60):\n",
    "    \"\"\"Create lag features and prepare data for model training.\"\"\"\n",
    "    # Ensure dataset has enough rows\n",
    "    if len(data) < look_back:\n",
    "        raise ValueError(f\"Not enough data to create lag features. Need at least {look_back} rows, but only have {len(data)}.\")\n",
    "    for i in range(1, look_back + 1):\n",
    "        data[f\"lag_{i}\"] = data['Close'].shift(i)\n",
    "    data.dropna(inplace=True)\n",
    "    # Ensure that we still have data after dropping NaNs\n",
    "    if data.empty:\n",
    "        raise ValueError(\"After applying lag features, no rows remain. Ensure the dataset has sufficient historical data.\")\n",
    "    # Define columns to exclude from training\n",
    "    columns_to_drop = ['Close', 'Date', 'title', 'combined_text']\n",
    "    X = data.drop(columns=[col for col in columns_to_drop if col in data.columns], errors='ignore')\n",
    "    y = data['Close']\n",
    "    # Ensure X is not empty before scaling\n",
    "    if X.empty:\n",
    "        raise ValueError(\"No valid feature data remaining after preprocessing. Check dataset integrity.\")\n",
    "    # Scale data\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    X_scaled = scaler_x.fit_transform(X)\n",
    "    y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n",
    "    return pd.DataFrame(X_scaled, columns=X.columns), pd.Series(y_scaled.ravel()), scaler_x, scaler_y\n",
    "\n",
    "def plot_feature_importance(model, X):\n",
    "    \"\"\"Plot feature importance of the trained model.\"\"\"\n",
    "    importance = model.feature_importances_\n",
    "    features = X.columns\n",
    "    sorted_idx = np.argsort(importance)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh([features[i] for i in sorted_idx[-10:]], importance[sorted_idx[-10:]])\n",
    "    plt.xlabel(\"Feature Importance\")\n",
    "    plt.title(\"Top 10 Important Features\")\n",
    "    plt.show()\n",
    "\n",
    "def execute(ticker, target_date):\n",
    "    \"\"\"Execute the stock prediction pipeline.\"\"\"\n",
    "    look_back = 60\n",
    "    # Fetch and process stock data\n",
    "    data = fetch_data(ticker)\n",
    "    # Fetch and process news sentiment\n",
    "    news = fetch_news(ticker)\n",
    "    news = extract_sentiment(news)\n",
    "    data = merge_news_with_data(data, news)\n",
    "    # Prepare data for model training\n",
    "    X, y, scaler_x, scaler_y = process_data(data, look_back)\n",
    "    # Train model\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X, y)\n",
    "    # Make prediction\n",
    "    target_data = X.iloc[-1].values.reshape(1, -1)\n",
    "    pred_scaled = model.predict(target_data)\n",
    "    pred = scaler_y.inverse_transform([[pred_scaled[0]]])[0, 0]\n",
    "    print(f\"\\nüìà Predicted closing price for {ticker} on {target_date}: **${pred:.2f}**\")\n",
    "    # Generate trading recommendation\n",
    "    last_known_price = scaler_y.inverse_transform([[y.iloc[-1]]])[0, 0]\n",
    "    if pred > last_known_price:\n",
    "        print(f\"üìä **Recommendation:** Buy a **CALL** option for {ticker} expiring on {target_date}.\")\n",
    "    elif pred < last_known_price:\n",
    "        print(f\"üìâ **Recommendation:** Buy a **PUT** option for {ticker} expiring on {target_date}.\")\n",
    "    else:\n",
    "        print(f\"‚öñÔ∏è No clear direction for {ticker} on {target_date}.\")\n",
    "    # Plot feature importance\n",
    "    plot_feature_importance(model, X)\n",
    "    # Display top 3 positive news articles\n",
    "    positive_news = news[news['title_sentiment'] > 0].sort_values(by=\"Date\", ascending=False).head(3)\n",
    "    print(\"\\nüîç **Top 3 Positive News Articles:**\")\n",
    "    for _, row in positive_news.iterrows():\n",
    "        print(f\"\\n **Title:** {row['title']}\")\n",
    "        print(f\" **Content:** {row['combined_text'][:200]}...\")  # Display first 200 characters\n",
    "# Run the model\n",
    "\n",
    "execute('TSLA', '2025-12-26')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching news from 2025-01-24 to 2025-01-31...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "‚ö†Ô∏è Error: After applying lag features, no rows remain. Reduce `look_back` or check dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py\u001b[0m in \u001b[0;36mline 151\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=396'>397</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m **Content:** \u001b[39m\u001b[39m{\u001b[39;00mrow[\u001b[39m'\u001b[39m\u001b[39mcombined_text\u001b[39m\u001b[39m'\u001b[39m][:\u001b[39m200\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)  \u001b[39m# Display first 200 characters\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=397'>398</a>\u001b[0m \u001b[39m# Run the model\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=399'>400</a>\u001b[0m execute(\u001b[39m'\u001b[39m\u001b[39mTSLA\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m2025-12-26\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py\u001b[0m in \u001b[0;36mline 124\u001b[0m, in \u001b[0;36mexecute\u001b[0;34m(ticker, target_date)\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=370'>371</a>\u001b[0m data \u001b[39m=\u001b[39m merge_news_with_data(data, news)\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=371'>372</a>\u001b[0m \u001b[39m# Prepare data for model training\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=372'>373</a>\u001b[0m X, y, scaler_x, scaler_y \u001b[39m=\u001b[39m process_data(data, look_back)\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=373'>374</a>\u001b[0m \u001b[39m# Train model\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=374'>375</a>\u001b[0m model \u001b[39m=\u001b[39m RandomForestRegressor(n_estimators\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
      "\u001b[1;32m/Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py\u001b[0m in \u001b[0;36mline 88\u001b[0m, in \u001b[0;36mprocess_data\u001b[0;34m(data, look_back)\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=334'>335</a>\u001b[0m \u001b[39m# Ensure we still have data after dropping NaNs\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=335'>336</a>\u001b[0m \u001b[39mif\u001b[39;00m data\u001b[39m.\u001b[39mempty:\n\u001b[0;32m---> <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=336'>337</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m‚ö†Ô∏è Error: After applying lag features, no rows remain. Reduce `look_back` or check dataset.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=337'>338</a>\u001b[0m \u001b[39m# Define columns to exclude from training\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=338'>339</a>\u001b[0m columns_to_drop \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mClose\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mDate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcombined_text\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: ‚ö†Ô∏è Error: After applying lag features, no rows remain. Reduce `look_back` or check dataset."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from newsapi import NewsApiClient\n",
    "from transformers import pipeline\n",
    "NEWS_API_KEY = \"ec064ce719114fe78bd3affdd71e5db8\"  # Replace with your actual API key\n",
    "# Initialize NewsAPI client\n",
    "\n",
    "newsapi = NewsApiClient(api_key=NEWS_API_KEY)\n",
    "# Initialize sentiment analysis using BERT\n",
    "\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
    "def fetch_data(ticker):\n",
    "    \"\"\"Fetch 20 years of historical stock data using yfinance.\"\"\"\n",
    "    t = yf.Ticker(ticker)\n",
    "    historical_data = t.history(period=\"20y\")\n",
    "    historical_data.reset_index(inplace=True)\n",
    "    historical_data['Date'] = historical_data['Date'].dt.tz_localize(None)\n",
    "    return historical_data\n",
    "\n",
    "def fetch_news(ticker):\n",
    "    \"\"\"Fetch recent news articles for the given stock ticker.\"\"\"\n",
    "    today = datetime.today().date()\n",
    "    # Set the max allowed date (from the NewsAPI error message)\n",
    "    max_allowed_date = datetime(2024, 12, 30).date()\n",
    "    # Adjust date range dynamically to avoid API errors\n",
    "    from_date = max(max_allowed_date, today - timedelta(days=7))\n",
    "    to_date = today\n",
    "    print(f\"Fetching news from {from_date} to {to_date}...\")\n",
    "    all_articles = newsapi.get_everything(\n",
    "        q=ticker,\n",
    "        from_param=from_date.strftime('%Y-%m-%d'),\n",
    "        to=to_date.strftime('%Y-%m-%d'),\n",
    "        language='en',\n",
    "        sort_by='relevancy'\n",
    "    )\n",
    "    news_data = all_articles.get('articles', [])\n",
    "    if not news_data:\n",
    "        print(f\"No articles found for {ticker}\")\n",
    "        return pd.DataFrame(columns=['Date', 'title', 'description', 'content'])\n",
    "    news_df = pd.DataFrame(news_data)\n",
    "    news_df['Date'] = pd.to_datetime(news_df['publishedAt']).dt.tz_localize(None)\n",
    "    return news_df[['Date', 'title', 'description', 'content']]\n",
    "\n",
    "def extract_sentiment(news_df):\n",
    "    \"\"\"Extract sentiment scores using BERT sentiment analysis.\"\"\"\n",
    "    if news_df.empty:\n",
    "        return news_df\n",
    "    news_df['combined_text'] = news_df['title'].fillna('') + ' ' + news_df['description'].fillna('') + ' ' + news_df['content'].fillna('')\n",
    "    news_df['sentiment_score'] = news_df['combined_text'].apply(lambda x: sentiment_analysis(x[:512])[0]['score'])  # Limit to 512 characters\n",
    "    news_df['sentiment_label'] = news_df['combined_text'].apply(lambda x: sentiment_analysis(x[:512])[0]['label'])\n",
    "    # Convert labels into numerical values\n",
    "    news_df['title_sentiment'] = news_df.apply(\n",
    "        lambda x: x['sentiment_score'] if x['sentiment_label'] == 'POSITIVE' else \n",
    "                  (-x['sentiment_score'] if x['sentiment_label'] == 'NEGATIVE' else 0),\n",
    "        axis=1\n",
    "    )\n",
    "    return news_df[['Date', 'title', 'title_sentiment', 'combined_text']]\n",
    "\n",
    "def merge_news_with_data(data, news):\n",
    "    \"\"\"Merge stock data with news sentiment scores.\"\"\"\n",
    "    if news.empty:\n",
    "        data['title_sentiment'] = 0\n",
    "        return data\n",
    "    merged_data = data.merge(news, on='Date', how='left')\n",
    "    merged_data.fillna({'title_sentiment': 0}, inplace=True)\n",
    "    # Weight sentiment by volume for impact\n",
    "    merged_data['weighted_sentiment'] = merged_data['title_sentiment'] * merged_data['Volume']\n",
    "    return merged_data\n",
    "\n",
    "def process_data(data, look_back=60):\n",
    "    \"\"\"Create lag features and prepare data for model training.\"\"\"\n",
    "    # Ensure dataset has enough rows before applying lags\n",
    "    if len(data) < look_back + 1:\n",
    "        print(f\"‚ö†Ô∏è Warning: Dataset has only {len(data)} rows. Reducing look_back to {max(1, len(data) - 1)}.\")\n",
    "        look_back = max(1, len(data) - 1)  # Adjust look_back dynamically\n",
    "    for i in range(1, look_back + 1):\n",
    "        data[f\"lag_{i}\"] = data['Close'].shift(i)\n",
    "    # Drop NaNs created by lag features\n",
    "    data.dropna(inplace=True)\n",
    "    # Ensure we still have data after dropping NaNs\n",
    "    if data.empty:\n",
    "        raise ValueError(\"‚ö†Ô∏è Error: After applying lag features, no rows remain. Reduce `look_back` or check dataset.\")\n",
    "    # Define columns to exclude from training\n",
    "    columns_to_drop = ['Close', 'Date', 'title', 'combined_text']\n",
    "    X = data.drop(columns=[col for col in columns_to_drop if col in data.columns], errors='ignore')\n",
    "    y = data['Close']\n",
    "    # Ensure X is not empty before scaling\n",
    "    if X.empty:\n",
    "        raise ValueError(\"‚ö†Ô∏è Error: No valid feature data remaining after preprocessing. Check dataset integrity.\")\n",
    "    # Scale data\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    X_scaled = scaler_x.fit_transform(X)\n",
    "    y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n",
    "    return pd.DataFrame(X_scaled, columns=X.columns), pd.Series(y_scaled.ravel()), scaler_x, scaler_y\n",
    "\n",
    "def plot_feature_importance(model, X):\n",
    "    \"\"\"Plot feature importance of the trained model.\"\"\"\n",
    "    importance = model.feature_importances_\n",
    "    features = X.columns\n",
    "    sorted_idx = np.argsort(importance)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh([features[i] for i in sorted_idx[-10:]], importance[sorted_idx[-10:]])\n",
    "    plt.xlabel(\"Feature Importance\")\n",
    "    plt.title(\"Top 10 Important Features\")\n",
    "    plt.show()\n",
    "\n",
    "def execute(ticker, target_date):\n",
    "    \"\"\"Execute the stock prediction pipeline.\"\"\"\n",
    "    look_back = 60\n",
    "    # Fetch and process stock data\n",
    "    data = fetch_data(ticker)\n",
    "    # Fetch and process news sentiment\n",
    "    news = fetch_news(ticker)\n",
    "    news = extract_sentiment(news)\n",
    "    data = merge_news_with_data(data, news)\n",
    "    # Prepare data for model training\n",
    "    X, y, scaler_x, scaler_y = process_data(data, look_back)\n",
    "    # Train model\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X, y)\n",
    "    # Make prediction\n",
    "    target_data = X.iloc[-1].values.reshape(1, -1)\n",
    "    pred_scaled = model.predict(target_data)\n",
    "    pred = scaler_y.inverse_transform([[pred_scaled[0]]])[0, 0]\n",
    "    print(f\"\\nüìà Predicted closing price for {ticker} on {target_date}: **${pred:.2f}**\")\n",
    "    # Generate trading recommendation\n",
    "    last_known_price = scaler_y.inverse_transform([[y.iloc[-1]]])[0, 0]\n",
    "    if pred > last_known_price:\n",
    "        print(f\"üìä **Recommendation:** Buy a **CALL** option for {ticker} expiring on {target_date}.\")\n",
    "    elif pred < last_known_price:\n",
    "        print(f\"üìâ **Recommendation:** Buy a **PUT** option for {ticker} expiring on {target_date}.\")\n",
    "    else:\n",
    "        print(f\"‚öñÔ∏è No clear direction for {ticker} on {target_date}.\")\n",
    "    # Plot feature importance\n",
    "    plot_feature_importance(model, X)\n",
    "    # Display top 3 positive news articles\n",
    "    positive_news = news[news['title_sentiment'] > 0].sort_values(by=\"Date\", ascending=False).head(3)\n",
    "    print(\"\\nüîç **Top 3 Positive News Articles:**\")\n",
    "    for _, row in positive_news.iterrows():\n",
    "        print(f\"\\n **Title:** {row['title']}\")\n",
    "        print(f\" **Content:** {row['combined_text'][:200]}...\")  # Display first 200 characters\n",
    "# Run the model\n",
    "\n",
    "execute('TSLA', '2025-12-26')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting xformers\n",
      "  Downloading xformers-0.0.29.post1.tar.gz (8.5 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=2.4 in /Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages (from xformers) (2.5.1)\n",
      "Requirement already satisfied: numpy in /Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages (from xformers) (1.24.3)\n",
      "Requirement already satisfied: filelock in /Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages (from torch>=2.4->xformers) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages (from torch>=2.4->xformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages (from torch>=2.4->xformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages (from torch>=2.4->xformers) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages (from torch>=2.4->xformers) (2023.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages (from torch>=2.4->xformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages (from sympy==1.13.1->torch>=2.4->xformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages (from jinja2->torch>=2.4->xformers) (2.1.1)\n",
      "Building wheels for collected packages: xformers\n",
      "  Building wheel for xformers (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m√ó\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m‚ï∞‚îÄ>\u001b[0m \u001b[31m[195 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m /Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/torch/utils/cpp_extension.py:497: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(msg.format('we could not find ninja.'))\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/_deprecation_warning.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/attn_bias_utils.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/checkpoint.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/test.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/utils.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/_cpp_lib.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/info.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/vararg_kernel.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/triton\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/residual.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/input_projection.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_mem_eff_attention.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_indexing.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_revnet.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_swiglu.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_merge_attentions.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_tiled_matmul.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/utils.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_nystrom_utils.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_attn_decoding.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_sequence_parallel_fused.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_sddmm.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_sp24.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_core.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/rmsnorm.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/modpar_layers.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/swiglu_op.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/unbind.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/rope_padded.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/seqpar.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/ipc.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/sequence_parallel_fused_ops.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/sp24.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/common.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/differentiable_collectives.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/tiled_matmul.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/indexing.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/device_limits.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/find_slowest.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/profiler_dcgm_impl.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/profiler_dcgm.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/api.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/profile_analyzer.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/profiler.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/_csr_ops.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/utils.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/blocksparse_tensor.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/csr_tensor.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/fused_softmax.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_blocksparse_attn_interface.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_blocksparse_attention.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/bert_padding.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton_og.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_interface.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/_sputnik_sparse.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/core.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/fourier_mix.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/scaled_dot_product.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/utils.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/attention_mask.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/attention_patterns.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/sparsity_config.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/base.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/batch_submit.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/batch_fetch_results.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/run_with_submitit.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/run_tasks.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/run_grid_search.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA/code\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/code/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA/code\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/code/model_wrapper.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA/code\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/code/dataset.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/benchmarks/LRA/code\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/_triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/_triton/k_scaled_index_add.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/_triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/_triton/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/_triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/_triton/matmul_perf_model.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/_triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/_triton/rope_padded_kernels.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/_triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/_triton/tiled_matmul_kernels.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/_triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/_triton/k_index_select_cat.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/_triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/_triton/rmsnorm_kernels.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/_triton\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/flash3.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/dispatch.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/attn_bias.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/ck.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/common.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/torch_attention_compat.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/ck_decoder.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/flash.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/cutlass.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/ck_splitk.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/triton_splitk.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha/_triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/_triton/splitk_kernels.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha/_triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/_triton/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/ops/fmha/_triton\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/losses\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/losses/cross_entropy.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/losses\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/losses/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/losses\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/layers/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/layers/patch_embed.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/layers/rotary.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton_amd/fwd_ref.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton_amd/interface_torch.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton_amd/fwd_prefill.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton_amd/interface_fa.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton_amd/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton_amd/test.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton_amd/bench.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton_amd/bwd_prefill.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton_amd/utils.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton_amd/fwd_decode.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton_amd/bwd_ref.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/pretrained.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/generation.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/benchmark.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/distributed.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/bigcode.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/gptj.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/opt.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/llama.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/vit.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/btlm.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/baichuan.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/bert.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/falcon.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/gpt_neox.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/gpt.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/activations.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/fused_dense.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/rms_norm.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/layer_norm.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/embedding.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/mlp.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/block.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/mha.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/triton/cross_entropy.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/triton/linear.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/triton/k_activations.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/triton/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/triton/mlp.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/triton/rotary.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/triton/layer_norm.py -> build/lib.macosx-11.1-arm64-cpython-311/xformers/_flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building 'xformers._C' extension\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-311\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-311/xformers\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/autograd\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/cpu\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/sequence_parallel_fused\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/sparse24\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/swiglu\n",
      "  \u001b[31m   \u001b[0m clang -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/ryanrodriguez/miniconda3/include -arch arm64 -fPIC -O2 -isystem /Users/ryanrodriguez/miniconda3/include -arch arm64 -I/private/var/folders/4c/8myr9q5j03bftqh6mvwgg5840000gn/T/pip-install-gmc_jn91/xformers_32fa4d7d40a848faaf8b7ced1d2a71b0/xformers/csrc -I/Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/torch/include -I/Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/torch/include/TH -I/Users/ryanrodriguez/miniconda3/lib/python3.11/site-packages/torch/include/THC -I/Users/ryanrodriguez/miniconda3/include/python3.11 -c xformers/csrc/attention/attention.cpp -o build/temp.macosx-11.1-arm64-cpython-311/xformers/csrc/attention/attention.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_clang\\\" -DPYBIND11_STDLIB=\\\"_libcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1002\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "  \u001b[31m   \u001b[0m clang: error: unsupported option '-fopenmp'\n",
      "  \u001b[31m   \u001b[0m error: command '/usr/bin/clang' failed with exit code 1\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for xformers\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for xformers\n",
      "Failed to build xformers\n",
      "\u001b[31mERROR: Could not build wheels for xformers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "isInteractiveWindowMessageCell": true
   },
   "source": [
    "Restarted base (Python 3.11.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Fetching stock data for TSLA...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fetch_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py\u001b[0m in \u001b[0;36mline 54\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=491'>492</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39müìù **Content:** \u001b[39m\u001b[39m{\u001b[39;00mrow[\u001b[39m'\u001b[39m\u001b[39mcombined_text\u001b[39m\u001b[39m'\u001b[39m][:\u001b[39m200\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)  \u001b[39m# Display first 200 characters\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=492'>493</a>\u001b[0m \u001b[39m# Run with debug mode\u001b[39;00m\n\u001b[0;32m---> <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=494'>495</a>\u001b[0m execute(\u001b[39m'\u001b[39m\u001b[39mTSLA\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m2025-12-26\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py\u001b[0m in \u001b[0;36mline 4\u001b[0m, in \u001b[0;36mexecute\u001b[0;34m(ticker, target_date, look_back)\n\u001b[1;32m      <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=442'>443</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Execute the stock prediction pipeline with debugging information.\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=443'>444</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39müöÄ Fetching stock data for \u001b[39m\u001b[39m{\u001b[39;00mticker\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=444'>445</a>\u001b[0m data \u001b[39m=\u001b[39m fetch_data(ticker)\n\u001b[1;32m      <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=445'>446</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m‚úÖ Stock data retrieved: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m rows\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=446'>447</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39müì∞ Fetching news for \u001b[39m\u001b[39m{\u001b[39;00mticker\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fetch_data' is not defined"
     ]
    }
   ],
   "source": [
    "def execute(ticker, target_date, look_back=60):\n",
    "    \"\"\"Execute the stock prediction pipeline with debugging information.\"\"\"\n",
    "    print(f\"\\nüöÄ Fetching stock data for {ticker}...\")\n",
    "    data = fetch_data(ticker)\n",
    "    print(f\"‚úÖ Stock data retrieved: {len(data)} rows\\n\")\n",
    "    print(f\"üì∞ Fetching news for {ticker}...\")\n",
    "    news = fetch_news(ticker)\n",
    "    print(f\"‚úÖ News data retrieved: {len(news)} articles\\n\")\n",
    "    print(\"üîç Extracting sentiment from news articles...\")\n",
    "    news = extract_sentiment(news)\n",
    "    print(f\"‚úÖ Sentiment analysis applied: {news.shape[0]} rows\\n\")\n",
    "    print(\"üîó Merging stock data with news sentiment...\")\n",
    "    data = merge_news_with_data(data, news)\n",
    "    print(f\"‚úÖ Data after merging: {data.shape}\\n\")\n",
    "    print(\"üìä Preview of merged data:\")\n",
    "    print(data.head())  # Show first few rows\n",
    "    print(\"\\n‚è≥ Processing data with lag features...\")\n",
    "    try:\n",
    "        X, y, scaler_x, scaler_y = process_data(data, look_back)\n",
    "    except ValueError as e:\n",
    "        print(f\"\\nüö® Error in process_data(): {e}\")\n",
    "        print(\"üìä Data before failing:\")\n",
    "        print(data.head())  # Show first few rows before error\n",
    "        return\n",
    "    print(f\"‚úÖ Processed data: {X.shape} features, {y.shape} targets\\n\")\n",
    "    print(\"üìä Preview of processed features:\")\n",
    "    print(X.head())  # Show first few processed rows\n",
    "    print(\"üõ† Training model...\")\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X, y)\n",
    "    print(\"‚úÖ Model training complete!\\n\")\n",
    "    print(\"üìà Predicting future price...\")\n",
    "    target_data = X.iloc[-1].values.reshape(1, -1)\n",
    "    pred_scaled = model.predict(target_data)\n",
    "    pred = scaler_y.inverse_transform([[pred_scaled[0]]])[0, 0]\n",
    "    print(f\"\\nüìä **Predicted closing price for {ticker} on {target_date}: ${pred:.2f}**\\n\")\n",
    "    last_known_price = scaler_y.inverse_transform([[y.iloc[-1]]])[0, 0]\n",
    "    if pred > last_known_price:\n",
    "        print(f\"üìä **Recommendation:** Buy a **CALL** option for {ticker} expiring on {target_date}.\")\n",
    "    elif pred < last_known_price:\n",
    "        print(f\"üìâ **Recommendation:** Buy a **PUT** option for {ticker} expiring on {target_date}.\")\n",
    "    else:\n",
    "        print(f\"‚öñÔ∏è No clear direction for {ticker} on {target_date}.\")\n",
    "    # Display feature importance\n",
    "    plot_feature_importance(model, X)\n",
    "    # Show top 3 positive news articles\n",
    "    positive_news = news[news['title_sentiment'] > 0].sort_values(by=\"Date\", ascending=False).head(3)\n",
    "    print(\"\\nüîç **Top 3 Positive News Articles:**\")\n",
    "    for _, row in positive_news.iterrows():\n",
    "        print(f\"\\nüìå **Title:** {row['title']}\")\n",
    "        print(f\"üìù **Content:** {row['combined_text'][:200]}...\")  # Display first 200 characters\n",
    "# Run with debug mode\n",
    "\n",
    "execute('TSLA', '2025-12-26')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching news from 2025-01-24 to 2025-01-31...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "‚ö†Ô∏è Error: After applying lag features, no rows remain. Reduce `look_back` or check dataset. Available rows: 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py\u001b[0m in \u001b[0;36mline 152\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=397'>398</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m **Content:** \u001b[39m\u001b[39m{\u001b[39;00mrow[\u001b[39m'\u001b[39m\u001b[39mcombined_text\u001b[39m\u001b[39m'\u001b[39m][:\u001b[39m200\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)  \u001b[39m# Display first 200 characters\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=398'>399</a>\u001b[0m \u001b[39m# Run the model\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=400'>401</a>\u001b[0m execute(\u001b[39m'\u001b[39m\u001b[39mTSLA\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m2025-12-26\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=401'>402</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexecute\u001b[39m(ticker, target_date, look_back\u001b[39m=\u001b[39m\u001b[39m60\u001b[39m):\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=402'>403</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Execute the stock prediction pipeline with debugging information.\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m/Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py\u001b[0m in \u001b[0;36mline 125\u001b[0m, in \u001b[0;36mexecute\u001b[0;34m(ticker, target_date)\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=371'>372</a>\u001b[0m data \u001b[39m=\u001b[39m merge_news_with_data(data, news)\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=372'>373</a>\u001b[0m \u001b[39m# Prepare data for model training\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=373'>374</a>\u001b[0m X, y, scaler_x, scaler_y \u001b[39m=\u001b[39m process_data(data, look_back)\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=374'>375</a>\u001b[0m \u001b[39m# Train model\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=375'>376</a>\u001b[0m model \u001b[39m=\u001b[39m RandomForestRegressor(n_estimators\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
      "\u001b[1;32m/Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py\u001b[0m in \u001b[0;36mline 89\u001b[0m, in \u001b[0;36mprocess_data\u001b[0;34m(data, look_back)\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=335'>336</a>\u001b[0m \u001b[39m# Ensure we still have data after dropping NaNs\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=336'>337</a>\u001b[0m \u001b[39mif\u001b[39;00m data\u001b[39m.\u001b[39mempty:\n\u001b[0;32m---> <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=337'>338</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m‚ö†Ô∏è Error: After applying lag features, no rows remain. Reduce `look_back` or check dataset. Available rows: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=338'>339</a>\u001b[0m \u001b[39m# Define columns to exclude from training\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/ryanrodriguez/Desktop/financialmodel/financialmodel.py?line=339'>340</a>\u001b[0m columns_to_drop \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mClose\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mDate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcombined_text\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: ‚ö†Ô∏è Error: After applying lag features, no rows remain. Reduce `look_back` or check dataset. Available rows: 0"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from newsapi import NewsApiClient\n",
    "from transformers import pipeline\n",
    "NEWS_API_KEY = \"ec064ce719114fe78bd3affdd71e5db8\"  # Replace with your actual API key\n",
    "# Initialize NewsAPI client\n",
    "\n",
    "newsapi = NewsApiClient(api_key=NEWS_API_KEY)\n",
    "# Initialize sentiment analysis using BERT\n",
    "\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
    "def fetch_data(ticker):\n",
    "    \"\"\"Fetch 20 years of historical stock data using yfinance.\"\"\"\n",
    "    t = yf.Ticker(ticker)\n",
    "    historical_data = t.history(period=\"20y\")\n",
    "    historical_data.reset_index(inplace=True)\n",
    "    historical_data['Date'] = historical_data['Date'].dt.tz_localize(None)\n",
    "    return historical_data\n",
    "\n",
    "def fetch_news(ticker):\n",
    "    \"\"\"Fetch recent news articles for the given stock ticker.\"\"\"\n",
    "    today = datetime.today().date()\n",
    "    # Set the max allowed date (from the NewsAPI error message)\n",
    "    max_allowed_date = datetime(2024, 12, 30).date()\n",
    "    # Adjust date range dynamically to avoid API errors\n",
    "    from_date = max(max_allowed_date, today - timedelta(days=7))\n",
    "    to_date = today\n",
    "    print(f\"Fetching news from {from_date} to {to_date}...\")\n",
    "    all_articles = newsapi.get_everything(\n",
    "        q=ticker,\n",
    "        from_param=from_date.strftime('%Y-%m-%d'),\n",
    "        to=to_date.strftime('%Y-%m-%d'),\n",
    "        language='en',\n",
    "        sort_by='relevancy'\n",
    "    )\n",
    "    news_data = all_articles.get('articles', [])\n",
    "    if not news_data:\n",
    "        print(f\"No articles found for {ticker}\")\n",
    "        return pd.DataFrame(columns=['Date', 'title', 'description', 'content'])\n",
    "    news_df = pd.DataFrame(news_data)\n",
    "    news_df['Date'] = pd.to_datetime(news_df['publishedAt']).dt.tz_localize(None)\n",
    "    return news_df[['Date', 'title', 'description', 'content']]\n",
    "\n",
    "def extract_sentiment(news_df):\n",
    "    \"\"\"Extract sentiment scores using BERT sentiment analysis.\"\"\"\n",
    "    if news_df.empty:\n",
    "        return news_df\n",
    "    news_df['combined_text'] = news_df['title'].fillna('') + ' ' + news_df['description'].fillna('') + ' ' + news_df['content'].fillna('')\n",
    "    news_df['sentiment_score'] = news_df['combined_text'].apply(lambda x: sentiment_analysis(x[:512])[0]['score'])  # Limit to 512 characters\n",
    "    news_df['sentiment_label'] = news_df['combined_text'].apply(lambda x: sentiment_analysis(x[:512])[0]['label'])\n",
    "    # Convert labels into numerical values\n",
    "    news_df['title_sentiment'] = news_df.apply(\n",
    "        lambda x: x['sentiment_score'] if x['sentiment_label'] == 'POSITIVE' else \n",
    "                  (-x['sentiment_score'] if x['sentiment_label'] == 'NEGATIVE' else 0),\n",
    "        axis=1\n",
    "    )\n",
    "    return news_df[['Date', 'title', 'title_sentiment', 'combined_text']]\n",
    "\n",
    "def merge_news_with_data(data, news):\n",
    "    \"\"\"Merge stock data with news sentiment scores.\"\"\"\n",
    "    if news.empty:\n",
    "        data['title_sentiment'] = 0\n",
    "        return data\n",
    "    merged_data = data.merge(news, on='Date', how='left')\n",
    "    merged_data.fillna({'title_sentiment': 0}, inplace=True)\n",
    "    # Weight sentiment by volume for impact\n",
    "    merged_data['weighted_sentiment'] = merged_data['title_sentiment'] * merged_data['Volume']\n",
    "    return merged_data\n",
    "\n",
    "def process_data(data, look_back=60):\n",
    "    \"\"\"Create lag features and prepare data for model training.\"\"\"\n",
    "    # Ensure dataset has enough rows before applying lags\n",
    "    if len(data) < look_back + 1:\n",
    "        print(f\"‚ö†Ô∏è Warning: Dataset has only {len(data)} rows. Reducing look_back to {max(1, len(data) - 1)}.\")\n",
    "        look_back = max(1, len(data) - 1)  # Adjust look_back dynamically\n",
    "    # Create lag features\n",
    "    for i in range(1, look_back + 1):\n",
    "        data[f\"lag_{i}\"] = data['Close'].shift(i)\n",
    "    # Drop NaNs created by lag features\n",
    "    data.dropna(inplace=True)\n",
    "    # Ensure we still have data after dropping NaNs\n",
    "    if data.empty:\n",
    "        raise ValueError(f\"‚ö†Ô∏è Error: After applying lag features, no rows remain. Reduce `look_back` or check dataset. Available rows: {len(data)}\")\n",
    "    # Define columns to exclude from training\n",
    "    columns_to_drop = ['Close', 'Date', 'title', 'combined_text']\n",
    "    X = data.drop(columns=[col for col in columns_to_drop if col in data.columns], errors='ignore')\n",
    "    y = data['Close']\n",
    "    # Ensure X is not empty before scaling\n",
    "    if X.empty:\n",
    "        raise ValueError(\"‚ö†Ô∏è Error: No valid feature data remaining after preprocessing. Check dataset integrity.\")\n",
    "    # Scale data\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    X_scaled = scaler_x.fit_transform(X)\n",
    "    y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n",
    "    return pd.DataFrame(X_scaled, columns=X.columns), pd.Series(y_scaled.ravel()), scaler_x, scaler_y\n",
    "\n",
    "def plot_feature_importance(model, X):\n",
    "    \"\"\"Plot feature importance of the trained model.\"\"\"\n",
    "    importance = model.feature_importances_\n",
    "    features = X.columns\n",
    "    sorted_idx = np.argsort(importance)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh([features[i] for i in sorted_idx[-10:]], importance[sorted_idx[-10:]])\n",
    "    plt.xlabel(\"Feature Importance\")\n",
    "    plt.title(\"Top 10 Important Features\")\n",
    "    plt.show()\n",
    "\n",
    "def execute(ticker, target_date):\n",
    "    \"\"\"Execute the stock prediction pipeline.\"\"\"\n",
    "    look_back = 60\n",
    "    # Fetch and process stock data\n",
    "    data = fetch_data(ticker)\n",
    "    # Fetch and process news sentiment\n",
    "    news = fetch_news(ticker)\n",
    "    news = extract_sentiment(news)\n",
    "    data = merge_news_with_data(data, news)\n",
    "    # Prepare data for model training\n",
    "    X, y, scaler_x, scaler_y = process_data(data, look_back)\n",
    "    # Train model\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X, y)\n",
    "    # Make prediction\n",
    "    target_data = X.iloc[-1].values.reshape(1, -1)\n",
    "    pred_scaled = model.predict(target_data)\n",
    "    pred = scaler_y.inverse_transform([[pred_scaled[0]]])[0, 0]\n",
    "    print(f\"\\nüìà Predicted closing price for {ticker} on {target_date}: **${pred:.2f}**\")\n",
    "    # Generate trading recommendation\n",
    "    last_known_price = scaler_y.inverse_transform([[y.iloc[-1]]])[0, 0]\n",
    "    if pred > last_known_price:\n",
    "        print(f\"üìä **Recommendation:** Buy a **CALL** option for {ticker} expiring on {target_date}.\")\n",
    "    elif pred < last_known_price:\n",
    "        print(f\"üìâ **Recommendation:** Buy a **PUT** option for {ticker} expiring on {target_date}.\")\n",
    "    else:\n",
    "        print(f\"‚öñÔ∏è No clear direction for {ticker} on {target_date}.\")\n",
    "    # Plot feature importance\n",
    "    plot_feature_importance(model, X)\n",
    "    # Display top 3 positive news articles\n",
    "    positive_news = news[news['title_sentiment'] > 0].sort_values(by=\"Date\", ascending=False).head(3)\n",
    "    print(\"\\nüîç **Top 3 Positive News Articles:**\")\n",
    "    for _, row in positive_news.iterrows():\n",
    "        print(f\"\\n **Title:** {row['title']}\")\n",
    "        print(f\" **Content:** {row['combined_text'][:200]}...\")  # Display first 200 characters\n",
    "# Run the model\n",
    "\n",
    "execute('TSLA', '2025-12-26')\n",
    "def execute(ticker, target_date, look_back=60):\n",
    "    \"\"\"Execute the stock prediction pipeline with debugging information.\"\"\"\n",
    "    print(f\"\\nüöÄ Fetching stock data for {ticker}...\")\n",
    "    data = fetch_data(ticker)\n",
    "    print(f\"‚úÖ Stock data retrieved: {len(data)} rows\\n\")\n",
    "    print(f\"üì∞ Fetching news for {ticker}...\")\n",
    "    news = fetch_news(ticker)\n",
    "    print(f\"‚úÖ News data retrieved: {len(news)} articles\\n\")\n",
    "    print(\"üîç Extracting sentiment from news articles...\")\n",
    "    news = extract_sentiment(news)\n",
    "    print(f\"‚úÖ Sentiment analysis applied: {news.shape[0]} rows\\n\")\n",
    "    print(\"üîó Merging stock data with news sentiment...\")\n",
    "    data = merge_news_with_data(data, news)\n",
    "    print(f\"‚úÖ Data after merging: {data.shape}\\n\")\n",
    "    print(\"üìä Preview of merged data:\")\n",
    "    print(data.head())  # Show first few rows\n",
    "    print(\"\\n‚è≥ Processing data with lag features...\")\n",
    "    try:\n",
    "        X, y, scaler_x, scaler_y = process_data(data, look_back)\n",
    "    except ValueError as e:\n",
    "        print(f\"\\nüö® Error in process_data(): {e}\")\n",
    "        print(\"üìä Data before failing:\")\n",
    "        print(data.head())  # Show first few rows before error\n",
    "        return\n",
    "    print(f\"‚úÖ Processed data: {X.shape} features, {y.shape} targets\\n\")\n",
    "    print(\"üìä Preview of processed features:\")\n",
    "    print(X.head())  # Show first few processed rows\n",
    "    print(\"üõ† Training model...\")\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X, y)\n",
    "    print(\"‚úÖ Model training complete!\\n\")\n",
    "    print(\"üìà Predicting future price...\")\n",
    "    target_data = X.iloc[-1].values.reshape(1, -1)\n",
    "    pred_scaled = model.predict(target_data)\n",
    "    pred = scaler_y.inverse_transform([[pred_scaled[0]]])[0, 0]\n",
    "    print(f\"\\nüìä **Predicted closing price for {ticker} on {target_date}: ${pred:.2f}**\\n\")\n",
    "    last_known_price = scaler_y.inverse_transform([[y.iloc[-1]]])[0, 0]\n",
    "    if pred > last_known_price:\n",
    "        print(f\"üìä **Recommendation:** Buy a **CALL** option for {ticker} expiring on {target_date}.\")\n",
    "    elif pred < last_known_price:\n",
    "        print(f\"üìâ **Recommendation:** Buy a **PUT** option for {ticker} expiring on {target_date}.\")\n",
    "    else:\n",
    "        print(f\"‚öñÔ∏è No clear direction for {ticker} on {target_date}.\")\n",
    "    # Display feature importance\n",
    "    plot_feature_importance(model, X)\n",
    "    # Show top 3 positive news articles\n",
    "    positive_news = news[news['title_sentiment'] > 0].sort_values(by=\"Date\", ascending=False).head(3)\n",
    "    print(\"\\nüîç **Top 3 Positive News Articles:**\")\n",
    "    for _, row in positive_news.iterrows():\n",
    "        print(f\"\\nüìå **Title:** {row['title']}\")\n",
    "        print(f\"üìù **Content:** {row['combined_text'][:200]}...\")  # Display first 200 characters\n",
    "# Run with debug mode\n",
    "\n",
    "execute('TSLA', '2025-12-26')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
